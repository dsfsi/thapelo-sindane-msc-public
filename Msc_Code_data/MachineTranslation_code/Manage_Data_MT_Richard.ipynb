{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a2914a-0048-4bb6-b96b-0c0bf5833bf6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1883893b-8a0d-494f-a8d7-b2d5499f19db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, GRU, TimeDistributed, Dense, Embedding, Reshape, Attention, Bidirectional, SpatialDropout1D, LSTM, Layer\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "from tensorflow import keras\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd960e4e-6f90-4d50-8b0f-ef82fafc74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting to use seaborn style for plots\n",
    "matplotlib.style.use(\"seaborn\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018ff08e-c4e7-4c65-8070-5d6f30904439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Wrapper_MT:\n",
    "       def __init__(self,root_data, root_mono, root_cross, root_bilex, path_to_destination_eda):\n",
    "                # initialize paths\n",
    "                self.source_data_p = root_data + \"/\" \n",
    "                self.target_data_p = root_data + \"/\" \n",
    "                self.src_trg_bilex = root_bilex + \"/\"\n",
    "                self.path_to_mono_emb = root_mono + \"/\" \n",
    "                self.path_to_cross_emb = root_cross + \"/\" \n",
    "                self.path_to_eda_output = path_to_destination_eda + \"/\"\n",
    "                \n",
    "       # load dataset\n",
    "       def load_source_target(self, source_lang_code, target_lang_code):\n",
    "                # get absolute source file\n",
    "                source_file  =  self.source_data_p + source_lang_code + \".txt\"\n",
    "                target_file  =  self.target_data_p + target_lang_code + \".txt\"\n",
    "                \n",
    "                # open source file\n",
    "                with open(source_file) as s_file:\n",
    "                         source_dataset = s_file.readlines()\n",
    "                        \n",
    "                # open target file\n",
    "                with open(target_file) as t_file:\n",
    "                         target_dataset = t_file.readlines()\n",
    "                         target_dataset = [\"sos \" + sentence + \" eos\" for sentence in target_dataset]\n",
    "                            \n",
    "                            \n",
    "                # open bilex file\n",
    "                src_bilex = []\n",
    "                trg_bilex = []\n",
    "                bilex_file = self.src_trg_bilex + source_lang_code + \"-\" + target_lang_code + \"/\" + source_lang_code + \"-\" + target_lang_code + \"-bilex.txt\" \n",
    "                with open(bilex_file) as b_file:\n",
    "                      for line in b_file:\n",
    "                              src_trg_split = line.split(\" ||| \")\n",
    "                              if src_trg_split[0] != \"UNK\" or src_trg_split[1] != \"UNK\":\n",
    "                                           src_bilex.append(src_trg_split[0])\n",
    "                                           \n",
    "                                           trg_bilex.append(\"sos \" +  src_trg_split[1].strip() + \" eos\") \n",
    "                                            \n",
    "                return source_dataset, target_dataset, src_bilex, trg_bilex\n",
    "       \n",
    "       # load embeddings\n",
    "       def load_embeddings(self, src_language, trg_language, sr_tokenizer, tr_tokenizer, cross_emb):\n",
    "                          if cross_emb:\n",
    "                                  # loading cross_lingual embeddings\n",
    "                                  s_t_folder = src_language + \"_\" + trg_language\n",
    "                                  s_t_file_src   = src_language + \"-\" + trg_language + \"-muse_src.txt\"\n",
    "                                  s_t_file_trg   = src_language + \"-\" + trg_language + \"-muse_trg.txt\"\n",
    "                                  \n",
    "                                  # TODO : Uncomment if statement when run starting\n",
    "                                  # if src_language == \"en\" or trg_language == \"en\":\n",
    "                                  #                   s_t_file_src = src_language + \"_\" + trg_language + \"-src-vcmp.txt\"\n",
    "                                  #                   s_t_file_trg = src_language + \"_\" + trg_language + \"-trg-vcmp.txt\"\n",
    "                                                    \n",
    "                                                    \n",
    "                                  src_x_emb_file_p = self.path_to_cross_emb + s_t_folder + \"/\" + s_t_file_src\n",
    "                                  trg_x_emb_file_p = self.path_to_cross_emb + s_t_folder + \"/\" + s_t_file_trg\n",
    "                                    \n",
    "                                  # load src cross emb\n",
    "                                  src_embeddings_index = {}\n",
    "                                  with open(src_x_emb_file_p) as f: \n",
    "                                                   for  line in f:\n",
    "                                                         word, coefs = line.split(maxsplit=1)\n",
    "                                                         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                                                         src_embeddings_index[word] = coefs\n",
    "                                  # print(\"Found %s word vectors.\" % len(embeddings_index) ) \n",
    "                                  #num_tokens = len(unique_t_d_words) + 1                                 # TODO\n",
    "                                  src_vocab = sr_tokenizer.word_index  \n",
    "                                  src_num_tokens = len(sr_tokenizer.word_index) + 1 \n",
    "                                  #print(src_num_tokens)  \n",
    "                                  embedding_dim =  200\n",
    "                                  hits_src = 0\n",
    "                                  misses_src = 0\n",
    "                                  hits_trg = 0\n",
    "                                  misses_trg = 0\n",
    "                                  input_size = 200\n",
    "                                  src_embedding_matrix = np.zeros((src_num_tokens, embedding_dim))\n",
    "                                  # for word, i in word2idx_all.items(): \n",
    "                                  for word in list(src_vocab.keys()):      \n",
    "                                                embedding_vector = src_embeddings_index.get(word)\n",
    "                                                if embedding_vector is not None:\n",
    "                                                    # src_embedding_matrix[i] = embedding_vector\n",
    "                                                    src_embedding_matrix[src_vocab[word]] = embedding_vector\n",
    "                                                    hits_src +=1\n",
    "                                                else:\n",
    "\n",
    "                                                    misses_src += 1               \n",
    "                                  #print(\"Converted %d words (%d misses)\" % (hits, misses)) \n",
    "\n",
    "                                  # load target embeddings \n",
    "                                  trg_embeddings_index = {}\n",
    "                                  with open(trg_x_emb_file_p) as f: \n",
    "                                                   for  line in f:\n",
    "                                                         word, coefs = line.split(maxsplit=1)\n",
    "                                                         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                                                         trg_embeddings_index[word] = coefs\n",
    "                                  # print(\"Found %s word vectors.\" % len(embeddings_index) ) \n",
    "                                  # num_tokens_trg = len(unique_t_d_words) + 1                               # TODO\n",
    "                                  trg_vocab = tr_tokenizer.word_index  \n",
    "                                  trg_num_tokens = len(tr_tokenizer.word_index) + 1  \n",
    "                                  trg_embedding_matrix = np.zeros((trg_num_tokens, embedding_dim))\n",
    "                                  # for word, i in word2idx_all.items():                                     # TODO \n",
    "                                  for word in list(trg_vocab.keys()): \n",
    "                                                embedding_vector = trg_embeddings_index.get(word)\n",
    "                                                if embedding_vector is not None:\n",
    "                                                    # trg_embedding_matrix[i] = embedding_vector\n",
    "                                                    trg_embedding_matrix[trg_vocab[word]] = embedding_vector\n",
    "                                                    hits_trg +=1\n",
    "                                                else:\n",
    "\n",
    "                                                    misses_trg += 1   \n",
    "                                  # convert to embeddings layer\n",
    "                                  src_embedding_layer = Embedding(src_num_tokens, embedding_dim,   embeddings_initializer = keras.initializers.Constant(src_embedding_matrix))\n",
    "                                  trg_embedding_layer = Embedding(trg_num_tokens, embedding_dim,  embeddings_initializer = keras.initializers.Constant(trg_embedding_matrix))\n",
    "                            \n",
    "                                    \n",
    "                                  return src_embedding_layer, trg_embedding_layer\n",
    "                                  \n",
    "                          else :\n",
    "                               # loading monolingual embeddings\n",
    "                               print(\"Mono\")\n",
    "         \n",
    "      \n",
    "             \n",
    "       # Augment dataset\n",
    "       def Data_Augment(self,source_dataset, target_dataset):\n",
    "           # augment machine translation with pseudo data\n",
    "           return augmented_source, augmented_target\n",
    "        \n",
    "        \n",
    "       # preprocess \n",
    "       def data_cleaning(self,):\n",
    "            \n",
    "            return data\n",
    "       \n",
    "       # basic EDA plotts\n",
    "       def plot_seq_length(self, df_series_src, df_series_trg, src_trg, kind=\"hist\"):\n",
    "\n",
    "            \"\"\" Plots length of all sentences in a pandas series\n",
    "\n",
    "            Args:\n",
    "                df_series (pd.Series): Pandas series containing text data.\n",
    "                kind (str): Type of visualisation. 'hist' or 'box'.\n",
    "\n",
    "            Returns:\n",
    "                Nothing\n",
    "\n",
    "            \"\"\"\n",
    "            new_eda_root_path = self.path_to_eda_output + src_trg[0] + \"-\" + src_trg[1] + \"/\"  \n",
    "            # save src plot\n",
    "            if kind == \"hist\":      \n",
    "                    os.mkdir(new_eda_root_path)\n",
    "                    \n",
    "            for pr in range(len(src_trg)):\n",
    "                   if pr == 0:\n",
    "                                if kind==\"hist\":\n",
    "                                    ax1 = df_series_src.str.split().map(lambda x: len(x)).hist(figsize=(10, 6))\n",
    "                                    ax1.set(xlabel=\"( Number of Tokens \" + src_trg[0] + \"-\" + src_trg[1] + \" )\", ylabel=\"( Number of Observations )\")\n",
    "                                    ax1.legend([src_trg[0]], loc='upper right')\n",
    "                                    plt.savefig(new_eda_root_path+src_trg[0]+ \".png\")\n",
    "                                    #plt.show()\n",
    "\n",
    "                                elif kind==\"box\":\n",
    "                                    ax3 = df_series_src.str.split().map(lambda x: len(x)).plot.box(figsize=(6,8))\n",
    "                                    ax3.set_ylabel(\"( Number of Tokens )\")\n",
    "                                    plt.savefig(new_eda_root_path+src_trg[0]+ \".png\")\n",
    "                                    #plt.show()\n",
    "                   else:\n",
    "                                if kind==\"hist\":\n",
    "                                    ax2 = df_series_trg.str.split().map(lambda x: len(x)).hist(figsize=(10, 6))\n",
    "                                    ax2.set(xlabel=\"( Number of Tokens \" + src_trg[0] + \"-\" + src_trg[1] + \" )\", ylabel=\"( Number of Observations )\")\n",
    "                                    ax2.legend([src_trg[1]],loc='lower right')\n",
    "                                    plt.savefig(new_eda_root_path+src_trg[1]+ \".png\") \n",
    "                                    #plt.show()\n",
    "\n",
    "                                elif kind==\"box\":\n",
    "                                        ax4 = df_series_trg.str.split().map(lambda x: len(x)).plot.box(figsize=(6,8))\n",
    "                                        ax4.set_ylabel(\"( Number of Tokens )\")\n",
    "                                        plt.savefig(new_eda_root_path+src_trg[1]+ \".png\")\n",
    "                                        #plt.show()\n",
    "                       \n",
    "\n",
    " \n",
    "        \n",
    "       \n",
    "       # data encoding\n",
    "       def encode_decode_tokenizers(self, source_datset, target_dataset):\n",
    "                      src_tokenizer = Tokenizer(oov_token=\"UNK\")\n",
    "                      src_tokenizer.fit_on_texts(source_datset)\n",
    "                      trg_tokenizer = Tokenizer(oov_token= \"UNK\")\n",
    "                      trg_tokenizer.fit_on_texts(target_dataset)\n",
    "\n",
    "                      return src_tokenizer, trg_tokenizer\n",
    "       \n",
    "       #\n",
    "       def sent_to_seq(self, sequences, tokenizer, vocab_size=None, reverse=False, onehot=False):\n",
    "\n",
    "                \"\"\" Converts text data into sequences supported by model input layers.\n",
    "\n",
    "                Args:\n",
    "                    sequences (list): List of text data.\n",
    "                    tokenizer (tf.keras.preprocessing.text.Tokenizer): Tensorflow tokenizer object.\n",
    "                    vocab_size (int): Number of words in the whole vocabulary.\n",
    "                    reverse (bool): Reverses the padded sequence if set True. Defaults False.\n",
    "                                    (Eg: if set True, [1 2 3 0 0] becomes [0 0 3 2 1])\n",
    "                    onehot (bool): Creates onehot representation of the padded sequence if set True.\n",
    "                                   Defaults False.\n",
    "\n",
    "                Returns:\n",
    "                    preprocessed_seq (list): List of preprocessed sequences.\n",
    "\n",
    "                \"\"\"\n",
    "               \n",
    "                # Tokenizing\n",
    "                seq = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "                # Padding\n",
    "                preprocessed_seq = pad_sequences(seq, padding='post', truncating='post', maxlen=200)\n",
    "\n",
    "                # Reversing\n",
    "                if reverse:\n",
    "                    preprocessed_seq = preprocessed_seq[:, ::-1]\n",
    "\n",
    "                # Onehot encoding\n",
    "                if onehot:\n",
    "                    preprocessed_seq = to_categorical(preprocessed_seq, num_classes=vocab_size) \n",
    "\n",
    "                return preprocessed_seq\n",
    "\n",
    " \n",
    "       def feature_extraction(self, X, y, src_tokenizer, trg_tokenizer, src_vocab_size, trg_vocab_size, batch_size):\n",
    "                    # source representations\n",
    "                    en_x = self.sent_to_seq(X, \n",
    "                                       tokenizer = src_tokenizer, \n",
    "                                       vocab_size = src_vocab_size,\n",
    "                                       onehot = False, \n",
    "                                       reverse = False)\n",
    "                    # target input representation\n",
    "                    de_inp = self.sent_to_seq(y,\n",
    "                                        tokenizer = trg_tokenizer,\n",
    "                                        vocab_size = trg_vocab_size,\n",
    "                                        onehot = False)\n",
    "                    # target representation  \n",
    "                    de_xy = self.sent_to_seq(y,\n",
    "                                        tokenizer = trg_tokenizer,\n",
    "                                        vocab_size = trg_vocab_size,\n",
    "                                        onehot = True)\n",
    "\n",
    "                    # de_x = de_xy[:,:-1,:]\n",
    "                    # de_y = de_xy[:,1:,:]\n",
    "                    \n",
    "                    return [en_x, de_inp], de_xy\n",
    "        \n",
    "        \n",
    "       \n",
    "       def generate_batch(self, X, y, src_tokenizer, trg_tokenizer, src_vocab_size, trg_vocab_size, batch_size):\n",
    "\n",
    "            \"\"\" Generator function to preprocess the data in batches and\n",
    "                feed them to the model.\n",
    "\n",
    "            Args:\n",
    "                X (list): List of text data.\n",
    "                y (list): List of text data.\n",
    "                batch_size (int): Number of items to be preprocessed and fed\n",
    "                                  in a single batch.\n",
    "            Yields:\n",
    "                en_x (list): Preprocessed data supported by encoder input layer.\n",
    "                de_x (list): Preprocessed data supported by decoder input layer.\n",
    "                            (1 timestep behind de_y, since \n",
    "                             TimeDistributed layer is being used for training)\n",
    "                de_y (list): Preprocessed actual outputs of decoder. \n",
    "                             (1 timestep ahead of de_x, since \n",
    "                             TimeDistributed layer is being used for training)\n",
    "            \"\"\"\n",
    "\n",
    "            while True:   \n",
    "                for batch in range(0, len(X), batch_size):\n",
    "#                     en_x = self.sent_to_seq(X[batch: batch+batch_size], \n",
    "#                                        tokenizer = src_tokenizer, \n",
    "#                                        vocab_size = src_vocab_size,\n",
    "#                                        onehot = True, \n",
    "#                                        reverse = True)\n",
    "\n",
    "#                     de_xy = self.sent_to_seq(y[batch: batch+batch_size],\n",
    "#                                         tokenizer = trg_tokenizer,\n",
    "#                                         vocab_size = trg_vocab_size,\n",
    "#                                         onehot = True)\n",
    "\n",
    "#                     de_x = de_xy[:,:-1,:]\n",
    "#                     de_y = de_xy[:,1:,:]\n",
    "                    \n",
    "                    \n",
    "                    # new data setup\n",
    "                    en_x = self.sent_to_seq(X, \n",
    "                                       tokenizer = src_tokenizer, \n",
    "                                       vocab_size = src_vocab_size,\n",
    "                                       onehot = False, \n",
    "                                       reverse = False)\n",
    "                    # target input representation\n",
    "                    de_inp = self.sent_to_seq(y,\n",
    "                                        tokenizer = trg_tokenizer,\n",
    "                                        vocab_size = trg_vocab_size,\n",
    "                                        onehot = False)\n",
    "                    # target representation  \n",
    "                    de_xy = self.sent_to_seq(y,\n",
    "                                        tokenizer = trg_tokenizer,\n",
    "                                        vocab_size = trg_vocab_size,\n",
    "                                        onehot = True)\n",
    "                    \n",
    "                    \n",
    "                    yield ([en_x, de_inp], de_xy)\n",
    "                    #return ([np.array(en_x), np.array(de_x)], np.array(de_y)\n",
    "                    # yield ([en_x.reshape(-1, en_x.shape[-1]), de_xy.reshape(-1,de_xy.shape[-1])], de_y)\n",
    "\n",
    "\n",
    "    \n",
    "       # create train test data split\n",
    "       def train_test_split(self,train_percent, dev_percent, source_data, target_data, bilex_src_data, bilex_trg_data, shuffle = False):\n",
    "                def is_what_percent_of(num_a, num_b):\n",
    "                            try:\n",
    "                                return int((num_a / 100 ) * num_b)\n",
    "                            except ZeroDivisionError:\n",
    "                                return 0\n",
    "                # get absolute source file\n",
    "                data_size = len(source_data)\n",
    "                data_size_bilex = len(bilex_src_data)\n",
    "                assert len(source_data) == len(target_data), \"Data size for source and target not the same\"\n",
    "                assert isinstance(train_percent, int), \"Please pass percent as int number in train percent param.\"\n",
    "                assert isinstance(dev_percent, int), \"Please pass percent as int number in dev percent param.\"\n",
    "                \n",
    "                cut_point = is_what_percent_of(train_percent,data_size)\n",
    "                if shuffle:\n",
    "                     print(\"Shuffling data first\")\n",
    "                else:\n",
    "                    # cut dataset\n",
    "                    \n",
    "                    #source train and test split\n",
    "                    source_train = source_data[:cut_point]\n",
    "                    source_test  = source_data[cut_point:]\n",
    "                    \n",
    "                    # target train test split\n",
    "                    target_train = target_data[:cut_point]\n",
    "                    target_test  = target_data[cut_point:]\n",
    "                    \n",
    "                    # get train dev split\n",
    "                    assert len(source_train) == len(target_train), \"Data size for source and target not the sae from train test divide\"\n",
    "                    assert len(source_test) > 0, \"There is no data available\"\n",
    "                    dev_cut_point = is_what_percent_of(dev_percent, len(source_train))\n",
    "                    \n",
    "                    source_train_cut  = source_train[:dev_cut_point]\n",
    "                    source_dev        = source_train[dev_cut_point:]\n",
    "                    \n",
    "                    target_train_cut  = target_train[:dev_cut_point]\n",
    "                    target_dev        = target_train[dev_cut_point:]\n",
    "                    \n",
    "                    assert len(source_train_cut) > 0 , \"No train data after dev split.\"\n",
    "                    \n",
    "                    # deal with bilex data\n",
    "                    #bilex_cut_point = cut_point\n",
    "                    #source train and test split bilex\n",
    "                    cut_point_bilex = is_what_percent_of(train_percent, data_size_bilex)\n",
    "                    bilex_source_train = bilex_src_data[:cut_point_bilex]\n",
    "                    bilex_source_test  = bilex_src_data[cut_point_bilex:]\n",
    "                    \n",
    "                    # target train test split\n",
    "                    bilex_target_train = bilex_trg_data[:cut_point_bilex]\n",
    "                    bilex_target_test  = bilex_trg_data[cut_point_bilex:]\n",
    "                    \n",
    "                    # get train dev split\n",
    "                    assert len(bilex_source_train) == len(bilex_target_train), \"Data size for source and target not the sae from train test divide in bilex\"\n",
    "                    assert len(bilex_source_test) > 0, \"There is no data available in bilex files\"\n",
    "                    dev_cut_point = is_what_percent_of(dev_percent,len(bilex_source_train))\n",
    "                    \n",
    "                    bilex_source_train_cut  = bilex_source_train[:dev_cut_point]\n",
    "                    bilex_source_dev        = bilex_source_train[dev_cut_point:]\n",
    "                    \n",
    "                    bilex_target_train_cut  = bilex_target_train[:dev_cut_point]\n",
    "                    bilex_target_dev        = bilex_target_train[dev_cut_point:]\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    return [source_train_cut + bilex_source_train_cut, source_dev + bilex_source_dev, source_test + bilex_source_test], [target_train_cut + bilex_target_train_cut, target_dev + bilex_target_dev, target_test + bilex_target_test]\n",
    "                    \n",
    "                        \n",
    "                #         \n",
    "\n",
    "#        def load_embeddings(self, src_lang, trg_lang, mono= False):\n",
    "#             # if mono_train\n",
    "#             if mono:\n",
    "#                 # loading monolingual embeddings\n",
    "#                 print(\"Setting up monoligual embeddings...\")\n",
    "#             else :\n",
    "#                         #loading cross-lingual embeddings\n",
    "#                         if src_lang == \"en\":\n",
    "#                                      path_to_cross_emb = \"\"\n",
    "#                         else:        \n",
    "#                                      path_cross_emb_src = self.path_to_cross_emb + \"/\" + src_lang + \"_\" + trg_lang + \"/\" + src_lang+\"-\"+ trg_lang+\"-\" + \"muse_src.txt\"\n",
    "#                                      path_cross_emb_trg = self.path_to_cross_emb + \"/\" + src_lang + \"_\" + trg_lang + \"/\" + src_lang+\"-\"+ trg_lang+\"-\" + \"muse_trg.txt\"\n",
    "\n",
    "\n",
    "#                                      return emb_matrix\n",
    "                    \n",
    "    \n",
    "       # prepare all the dataset for source abd target for taining                \n",
    "       def do_prepare(self,):\n",
    "            \n",
    "            \n",
    "           return  train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9164c742-2775-4dd5-ad6d-836772d9b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_input = tf.convert_to_tensor(np.random.rand(10, 20))\n",
    "# # embedding_dim = np.random.rand(200)\n",
    "# trg_input = tf.convert_to_tensor(np.random.rand(10, 20))\n",
    "# trg_out = tf.convert_to_tensor(np.random.rand(10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "819bbff1-4968-4dee-bac7-3ff681c5b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Wrapper:\n",
    "    def __init__(self, s_seq_len,t_seq_len, s_vocab_size, t_vocab_len, s_model_units, t_model_units):\n",
    "           self.src_seq_len = s_seq_len\n",
    "           self.src_vocab_size  = s_vocab_size\n",
    "           self.model_enc_units_src = s_model_units\n",
    "            \n",
    "           self.trg_seq_len =  t_seq_len\n",
    "           self.trg_vocab_size = t_vocab_len\n",
    "           self.model_enc_units_trg  = t_model_units \n",
    "            \n",
    "    def Encoder_Decoder(self, src_emb_layer, trg_emb_layer):\n",
    "        # Encoder\n",
    "        # self.trg_seq_len \n",
    "        src_inputs = Input(shape = (self.src_seq_len,)) # [None, 20]\n",
    "        src_embedding_sequences = src_emb_layer(src_inputs) # [None, 20, 200]\n",
    "        src_attention_layer = Attention(use_scale=False)([src_embedding_sequences, src_embedding_sequences])\n",
    "        src_gru = GRU(self.model_enc_units_src, return_sequences=True, return_state=True)\n",
    "        src_out, src_state = src_gru(src_attention_layer)\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        # self.trg_seq_len - 1\n",
    "        trg_inputs = Input(shape=(self.trg_seq_len,))\n",
    "        trg_embedding_sequences = trg_emb_layer(trg_inputs)\n",
    "        trg_attention_layer = Attention(use_scale=False)([trg_embedding_sequences, trg_embedding_sequences])\n",
    "        trg_gru = GRU(self.model_enc_units_trg, return_sequences=True, return_state=True)\n",
    "        trg_out, hidden_d = trg_gru(trg_attention_layer, initial_state = src_state)\n",
    "        trg_dense = TimeDistributed(Dense(self.trg_vocab_size, activation='softmax'))    \n",
    "        trg_pred = trg_dense(trg_out)\n",
    "        \n",
    "        # model compiling\n",
    "        nmt = Model(inputs=[src_inputs, trg_inputs], outputs = trg_pred) \n",
    "        nmt.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "        \n",
    "        print(nmt.summary())\n",
    "        return nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1bf5b-191f-4592-97d6-cf65002180de",
   "metadata": {},
   "source": [
    "# Inference class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5fb0af9-b23a-4356-8ecc-ba3f127008a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "       def __init__(self, model_destination, tokenizer_destination, translation_doc_destination, cross_results_destination):\n",
    "               self.m_destination = model_destination + \"/\"\n",
    "               self.tok_destination = tokenizer_destination + \"/\"\n",
    "               self.trans_destination = translation_doc_destination + \"/\"\n",
    "               self.x_emb_results     = cross_results_destination + \"/\"\n",
    "            \n",
    "            \n",
    "       # save model\n",
    "       def save_models(self, encoder, decoder, src_lang, trg_lang):\n",
    "                new_model_root_path = self.m_destination + src_lang + \"-\" + trg_lang + \"/\"\n",
    "                os.mkdir(new_model_root_path)\n",
    "                # Encoder\n",
    "                save_model(encoder, new_model_root_path)\n",
    "\n",
    "                # Decoder\n",
    "                save_model(decoder, new_model_root_path)\n",
    "\n",
    "\n",
    "              \n",
    "            \n",
    "            \n",
    "       # save tokenizers\n",
    "       def save_tokenizers(self,src_tok, trg_tok, src_lang, trg_lang):\n",
    "                new_eda_root_path = self.tok_destination + src_lang + \"-\" + trg_lang + \"/\"\n",
    "                os.mkdir(new_eda_root_path)\n",
    "                # Src\n",
    "                src_tok_path = new_eda_root_path + src_lang + \"-Tokenizer.pkl\"\n",
    "                eng_tok_file = open(src_tok_path, \"wb\")\n",
    "                pickle.dump(eng_tok, eng_tok_file)\n",
    "\n",
    "                # Trg\n",
    "                trg_tok_path = new_eda_root_path + trg_lang + \"-Tokenizer.pkl\"\n",
    "                it_tok_file = open( trg_tok_path, \"wb\")\n",
    "                pickle.dump(it_tok, it_tok_file)\n",
    "                \n",
    "       # word to one hot\n",
    "       def word_to_onehot(self, tokenizer, word, vocab_size):\n",
    "                \"\"\" Converts a single word into onehot representation.\n",
    "\n",
    "                Args:\n",
    "                    tokenizer (tf.keras.preprocessing.text.Tokenizer): Tensorflow tokenizer object.\n",
    "                    word (str): Word to be tokenized and onehot encoded.\n",
    "                    vocab_size (int): Number of words in the whole vocabulary.\n",
    "\n",
    "                Returns:\n",
    "                    de_onhot (list): Onehot representation of given word.\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "                de_seq = tokenizer.texts_to_sequences([[word]])\n",
    "                de_onehot = to_categorical(de_seq, num_classes=vocab_size).reshape(1, 1, vocab_size)  \n",
    "\n",
    "                return de_onehot\n",
    "       \n",
    "       # translate test data\n",
    "       def translate(self, data_loader_class, encoder, decoder, src_sentence, src_tokenizer, trg_tokenizer, src_vocab_size, trg_vocab_size, trg_seq_len):\n",
    "                    \"\"\" Returns Target translation of given Source sentence.\n",
    "\n",
    "                    Args:\n",
    "                        src_sentence (str): Source text to be translated.\n",
    "\n",
    "                    Returns:\n",
    "                        trg_sent (str): Target translated text.\n",
    "\n",
    "                    \"\"\"\n",
    "\n",
    "                    src_seq = data_loader_class.sent_to_seq([src_sentence], \n",
    "                                         tokenizer=src_tokenizer, \n",
    "                                         reverse=True, \n",
    "                                         onehot=True, \n",
    "                                         vocab_size=src_vocab_size)\n",
    "\n",
    "                    trg_st = encoder.predict(src_seq)\n",
    "                    trg_seq = self.word_to_onehot(trg_tokenizer, \"sos\", trg_vocab_size)\n",
    "                    trg_sent = \"\"\n",
    "                    # for i in range(trg_seq_len):  \n",
    "                    for i in range(20):    \n",
    "                        de_prob, en_st = decoder.predict([trg_seq, trg_st])\n",
    "                        index = np.argmax(de_prob[0, :], axis=-1)\n",
    "                        de_w = trg_tokenizer.index_word[index]\n",
    "                        trg_seq = self.word_to_onehot(trg_tokenizer, de_w, trg_vocab_size) \n",
    "                        if de_w == 'eos': break\n",
    "                        trg_sent += de_w + ' '\n",
    "\n",
    "                    return trg_sent \n",
    "\n",
    "       # save translation document     \n",
    "       def create_translation_doc(self, translations, src_lang, trg_lang):\n",
    "                      new_doc_path = self.tok_destination + src_lang + \"-\" + trg_lang + \"/\"\n",
    "                      os.mkdir(new_doc_path)\n",
    "                      new_file_created = new_doc_path + src_lang + \"-\" + trg_lang + \".txt\"\n",
    "                      with open(new_file_created) as new_fdile:\n",
    "                                  for trans in translations:\n",
    "                                                 new_fdile.write(\"Src sent: \" + trans[0] + \"\\n\")\n",
    "                                                 new_fdile.write(\"Translated text :\" + trans[1] + \"\\n\")\n",
    "                                 \n",
    "                             \n",
    "               \n",
    "       # append to cross_lingual results document\n",
    "       def add_translation_results(self, src_lang,trg_lang, loss, acc):\n",
    "                    apended_file_loss_and_acc = self.x_emb_results + \"X_emb_loss_and_acc.txt\"\n",
    "                    with open(apended_file_loss_and_acc, \"a\") as ap_file:\n",
    "                                   ap_file.write(\"Langs:\" + \" \" + src_lang+\"-\"+trg_lang+ \" \" + \"Loss:\" + loss + \" \" + \"Accuracy:\" + \" \" + acc + \"\\n\")\n",
    "                    \n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448b5d5-6091-4de5-a2d3-acf368cf0e7b",
   "metadata": {},
   "source": [
    "# Training and evaluation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2634a1-17de-4fb9-929d-6583421dd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, root_dir, src_lang, trg_lang, metric = \"loss\"):\n",
    "    \n",
    "    \"\"\" Plots training history of models.\n",
    "    \n",
    "    Args:\n",
    "        history (History): Tensorflow History object containing training \n",
    "                           history of the model.\n",
    "        metric (str): 'accuracy' or 'loss'. Metric to be plotted. \n",
    "                           \n",
    "    Returns:\n",
    "        Nothing\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    history_dict = history.history\n",
    "    epochs = range(1, len(history_dict['loss'])+1)\n",
    "    new_eda_root_path = roo_dir + \"/\" + src_lang + \"-\" + trg_lang + \"/\"\n",
    "    if metric == \"loss\":\n",
    "        file = new_eda_root_path + src_lang + \"-\" + trg_lang + \"-loss.eps\"\n",
    "        os.mkdir(new_eda_root_path)\n",
    "        train_loss = history_dict['loss']    # Training loss over epochs\n",
    "        val_loss = history_dict['val_loss']    # Validation loss over epochs\n",
    "        plt.plot(epochs, train_loss,'b', label='Training error')\n",
    "        plt.plot(epochs, val_loss,'b', color=\"orange\", label='Validation error')\n",
    "        plt.title('Training and Validation error')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Error')\n",
    "        plt.legend()\n",
    "        plt.savefig(file, format = \"eps\", dpi=350)\n",
    "        #plt.show()\n",
    "        # save image\n",
    "        \n",
    "    \n",
    "    elif metric == \"accuracy\":\n",
    "        file = new_eda_root_path + src_lang + \"-\" + trg_lang + \"-accuracy.eps\"\n",
    "        train_acc = history_dict['acc']    # Training accuracy over epochs\n",
    "        val_acc = history_dict['val_acc']    # Validation accuracy over epochs\n",
    "        plt.plot(epochs, train_acc,'b', label='Training accuracy')\n",
    "        plt.plot(epochs, val_acc,'b', color=\"orange\", label='Validation accuracy')\n",
    "        plt.title('Training and Validation accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.savefig(file, format = \"eps\", dpi=350)\n",
    "        # plt.show()\n",
    "        # save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d948a31-7a9a-4b18-924e-1300f652f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings layer\n",
      "The sizes of source language af and target language en --> train 6348, dev 1588, and test 1985\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 200, 200)     2673400     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 200, 200)     2283400     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 200, 200)     0           embedding_4[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 200, 200)     0           embedding_5[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     [(None, 200, 100), ( 90300       attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_5 (GRU)                     [(None, 200, 100), ( 90300       attention_5[0][0]                \n",
      "                                                                 gru_4[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 200, 11417)   1153117     gru_5[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 6,290,517\n",
      "Trainable params: 6,290,517\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 54.0 GiB for an array with shape (1269600, 11417) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f804509908ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                                       \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                                                       \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_train_dev_test_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_train_dev_test_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_data_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_data_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_data_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_data_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                                                       validation_steps = len(src_train_dev_test_split[1])//batch_size)\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;34m'Keras requires a thread-safe generator when '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             '`use_multiprocessing=False, workers > 1`. ')\n\u001b[0;32m--> 922\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m   \"\"\"\n\u001b[0;32m--> 832\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-929e2985a45f>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(self, X, y, src_tokenizer, trg_tokenizer, src_vocab_size, trg_vocab_size, batch_size)\u001b[0m\n\u001b[1;32m    305\u001b[0m                                         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                                         \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                                         onehot = True)\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-929e2985a45f>\u001b[0m in \u001b[0;36msent_to_seq\u001b[0;34m(self, sequences, tokenizer, vocab_size, reverse, onehot)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# Onehot encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0monehot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mpreprocessed_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessed_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow_core/python/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 54.0 GiB for an array with shape (1269600, 11417) and data type float32"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f1197dda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_plots_dir = \"./Loss_Acc_train_plots\"\n",
    "encoder_seq_len = 5\n",
    "decoder_seq_len = 5\n",
    "gru_units = 10    # Number of GRU units\n",
    "n_epochs, batch_size = 1, 4\n",
    "if __name__ == '__main__':\n",
    "            # This code won't run if this file is imported.\n",
    "            #initialize object\n",
    "            data_wrapper = Data_Wrapper_MT(\"./MT-datasets\", \"./Monolinguals\", \"./M2M-CL-Embeddings\", \"./Bilexicons_Folder\", \"./EDA_Destination_Folder\")\n",
    "            inference_wrapper = Inference(\"./Saved_X_lingual_MT_Models\", \"./Saved_lang_Tokenizers\", \"./Test_Translation_Documents\", \"Loss_Acc_Document\")\n",
    "            list_langs = ['af', 'xho', 'zul', 'ssw', 'nr', 'nso', 'st', 'tsn', 'ven', 'tso', 'en']\n",
    "            results_pairs = [(i, j) for i in list_langs for j in list_langs if i != j]\n",
    "            for pair in results_pairs:\n",
    "                         if pair[0] == \"en\" and pair[1] == \"xho\" :\n",
    "                                    # load datasets\n",
    "                                    source_d_text, target_d_text, src_bilex_text, trg_bilex_text = data_wrapper.load_source_target(pair[0], pair[1])\n",
    "                                    # Basic EDA plots source and target bar\n",
    "                                    eda_plots = data_wrapper.plot_seq_length(pd.Series(source_d_text + src_bilex_text), pd.Series(target_d_text + trg_bilex_text), pair)\n",
    "                                    # Basic EDA kind box\n",
    "                                    # eda_plots_2 = data_wrapper.plot_seq_length(pd.Series(source_d_text + src_bilex_text), pd.Series(target_d_text + trg_bilex_text), pair, kind=\"box\")\n",
    "                                    # leartn tokenizers\n",
    "                                    src_data_tokenizer, trg_data_tokenizer = data_wrapper.encode_decode_tokenizers(source_d_text + src_bilex_text,  target_d_text + trg_bilex_text )\n",
    "                                    \n",
    "                                    # initialize cross embeddings layers\n",
    "                                    print(\"Initializing embeddings layer\")\n",
    "                                    src_emb_layer, trg_emb_layer = data_wrapper.load_embeddings(pair[0], pair[1], src_data_tokenizer, trg_data_tokenizer, True)\n",
    "                                    \n",
    "                                    # train_test_split\n",
    "                                    \"\"\"\n",
    "                                    split data into train, dev and test with test = 20 %, and train = 80 % from test split. Therefore dev = 20 % of train split\n",
    "                                    \"\"\"\n",
    "                                    src_train_dev_test_split, trg_train_dev_test_split = data_wrapper.train_test_split(80, 80, source_d_text, target_d_text, src_bilex_text, trg_bilex_text)\n",
    "                                    print(f\"The sizes of source language {pair[0]} and target language {pair[1]} --> train {len(src_train_dev_test_split[0])}, dev {len(src_train_dev_test_split[1])}, and test {len(src_train_dev_test_split[2])}\")\n",
    "                                    # print(f\"The size of target language {pair[1]} --> train {len(trg_train_dev_test_split[0])}, dev {len(trg_train_dev_test_split[1])}, and test {len(trg_train_dev_test_split[2])}\")\n",
    " \n",
    "\n",
    "                                    #initialize model object Model + \"_\" + pair[0] + \"_\" pair[1] \n",
    "                                    Model_src_trg_obj = Model_Wrapper(encoder_seq_len, decoder_seq_len, len(src_data_tokenizer.word_index) + 1 , len(trg_data_tokenizer.word_index) + 1 ,gru_units, gru_units)\n",
    "                                    nmt = Model_src_trg_obj.Encoder_Decoder(src_emb_layer, trg_emb_layer)\n",
    "                                    \n",
    "                                    \n",
    "                                    # Old Training and validation\n",
    "                                    history = nmt.fit(data_wrapper.generate_batch(src_train_dev_test_split[0], trg_train_dev_test_split[0], src_data_tokenizer, trg_data_tokenizer, len(src_data_tokenizer.word_index) + 1 , len(trg_data_tokenizer.word_index) + 1 , batch_size=batch_size),\n",
    "                                                      steps_per_epoch = len(src_train_dev_test_split[0])//batch_size,\n",
    "                                                      epochs = n_epochs,\n",
    "                                                      validation_data = data_wrapper.generate_batch(src_train_dev_test_split[1], trg_train_dev_test_split[1], src_data_tokenizer, trg_data_tokenizer,len(src_data_tokenizer.word_index) + 1 , len(trg_data_tokenizer.word_index) + 1 , batch_size=batch_size),\n",
    "                                                      validation_steps = len(src_train_dev_test_split[1])//batch_size)\n",
    "\n",
    "                                    \n",
    "                                    plot_history(history,root_plots_dir, pair[0], pair[1])\n",
    "                                    plot_history(history, metric=\"accuracy\")\n",
    "\n",
    "                                    # save the model\n",
    "                                    \n",
    "\n",
    "                                    # Evaluating on completely unseen data\n",
    "                                    X_test, y_test     =  data_wrapper.feature_extraction(src_train_dev_test_split[2], trg_train_dev_test_split[2], src_data_tokenizer, trg_data_tokenizer,len(src_data_tokenizer.word_index) + 1 , len(trg_data_tokenizer.word_index) + 1 , batch_size=batch_size)\n",
    "\n",
    "                                    \n",
    "                                    results = nmt.evaluate([X_test[0], X_test[1]], y_test, steps=batch_size)\n",
    "                                    print(f\"========== Test set loss: {results[0]} - Test set Accuracy: {results[1]*100} % ==========\")\n",
    "                                    \n",
    "                                    \"\"\"\n",
    "                                    TODO - Richard \n",
    "                                    check every line below this comment and edit where necessary. \n",
    "                                    \"\"\"\n",
    "                                    \n",
    "                                    # Get Model Weights encoder\n",
    "                                    attention_en_wei = nmt.layers[4].get_weights()\n",
    "                                    gru_en_weights   = nmt.layers[6].get_weights()\n",
    "                                    \n",
    "                                    \n",
    "                                    # redefine models\n",
    "                                    # The Envoder definition\n",
    "                                    en_inputs = Input(shape= (encoder_seq_len,))\n",
    "                                    src_embeddings  =  src_emb_layer(en_inputs)\n",
    "                                    src_attention  = Attention(use_scale=False)([src_embeddings, src_embeddings])\n",
    "                                    src_gru = GRU(gru_units, return_sequences=True, return_state=True)\n",
    "                                    src_out, src_state = src_gru(src_attention)\n",
    "                                    encoder = Model(inputs=en_inputs, outputs=en_state)\n",
    "                                    \n",
    "                                    # set weights\n",
    "                                    src_attention.set_weights(attention_en_wei)\n",
    "                                    src_gru.set_weights(gru_en_weights)\n",
    "                                    \n",
    "                                    # get decoder weights\n",
    "                                    attention_de_wei = nmt.layers[5].get_weights()\n",
    "                                    gru_de_weights   = nmt.layers[7].get_weights()\n",
    "                                    dense_de_wei     = nmt.layers[8].get_weights()\n",
    "                                    \n",
    "                                    # The decoder definition\n",
    "                                    de_inputs = Input(shape= (decoder_seq_len,))\n",
    "                                    trg_embeddings  =  trg_emb_layer(de_inputs)\n",
    "                                    trg_attention  = Attention(use_scale=False)([trg_embeddings, trg_embeddings])\n",
    "                                    trg_gru         =  GRU(gru_units, return_sequences=True, return_state=True)\n",
    "                                    trg_out, trg_state = en_gru(trg_attention)\n",
    "                                    de_dense = Dense(trg_vocab_size, activation='softmax') \n",
    "                                    de_pred = de_dense(de_out)\n",
    "                                    decoder = Model(inputs=[de_inputs, de_state_in], outputs=[de_pred, trg_state])\n",
    "                                    \n",
    "                                    # set weights\n",
    "                                    trg_attention.set_weights(attention_de_wei)\n",
    "                                    trg_gru.set_weights(gru_de_weights)\n",
    "                                    de_dense.set_weights(dense_de_wei)\n",
    "                                    \n",
    "                                    # use model to translate\n",
    "                                    src_trg_collection = []\n",
    "                                    for src_sentence in X_test[0][len(X_test) - 25:]:\n",
    "                                                 src_trg_pairs = []\n",
    "                                                 trg_translation = inference_wrapper.translate(data_wrapper, encoder, decoder, src_sentence, src_tokenizer, trg_tokenizer, src_vocab_size, trg_vocab_size, trg_seq_len)\n",
    "                                                 src_trg_pairs.append(src_sentence)\n",
    "                                                 src_trg_pairs.append(trg_translation)\n",
    "                                                 src_trg_collection.append(src_trg_pairs)\n",
    "                                                \n",
    "                                    # save to folder\n",
    "                                    inference_wrapper.create_translation_doc(src_trg_collection, pair[0], pair[1])\n",
    "                                    inference_wrapper.add_translation_results(pair[0], pair[1], results[0], results[1])\n",
    "                                    \n",
    "                                    # save encoder decoder\n",
    "                                    inference_wrapper.save_models(ecoder,decoder, pair[0], pair[1])\n",
    "                                    \n",
    "                                    # save tokenizers\n",
    "                                    inference_wrapper.save_tokenizers(src_data_tokenizer, trg_data_tokenizer, pair[0], pair[1])\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb16367-317f-4c6a-bc8e-710e16f0d107",
   "metadata": {},
   "source": [
    "# Toy examples to get layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7d28704-69c8-44bb-80a4-92dc3e4a23b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_embedding_sequences = Embedding(13367, 200, embeddings_initializer=\"uniform\")\n",
    "# trg_embedding_sequences = Embedding(11417, 200, embeddings_initializer=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f5d71ca-0702-48f3-89bc-f110c6e8e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # self.trg_seq_len \n",
    "# src_inputs = Input(shape = (20,)) # [None, 20]\n",
    "# src_embedding = src_embedding_sequences(src_inputs)\n",
    "# src_attention_layer = Attention(use_scale=False)([src_embedding, src_embedding])\n",
    "# src_gru = GRU(5, return_sequences=True, return_state=True)\n",
    "# src_out, src_state = src_gru(src_attention_layer)\n",
    "\n",
    "# # Decoder\n",
    "# # self.trg_seq_len - 1\n",
    "# trg_inputs = Input(shape=(20,))\n",
    "# trg_embedding = trg_embedding_sequences(trg_inputs)\n",
    "# trg_attention_layer = Attention(use_scale=False)([trg_embedding, trg_embedding])\n",
    "# trg_gru = GRU(5, return_sequences=True, return_state=True)\n",
    "# trg_out, hidden_d = trg_gru(trg_attention_layer, initial_state = src_state)\n",
    "# trg_dense = TimeDistributed(Dense(11417, activation='softmax'))    \n",
    "# trg_pred = trg_dense(trg_out)\n",
    "\n",
    "# # model compiling\n",
    "# nmt = Model(inputs=[src_inputs, trg_inputs], outputs = trg_pred) \n",
    "# nmt.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85d5a986-c173-4ba1-916f-0fb1b6f74df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f78aa8143a0>,\n",
       " <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f77b85aa9a0>,\n",
       " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f77b86a0100>,\n",
       " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f77b86a0040>,\n",
       " <tensorflow.python.keras.layers.dense_attention.Attention at 0x7f78aa814580>,\n",
       " <tensorflow.python.keras.layers.dense_attention.Attention at 0x7f78aa7ec370>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.GRU at 0x7f77b8645fd0>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.GRU at 0x7f78aa818af0>,\n",
       " <tensorflow.python.keras.layers.wrappers.TimeDistributed at 0x7f77d81098b0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nmt.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e430b16-1633-4aef-a8f7-61864d854821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_wrapper = Data_Wrapper_MT(\"../../MT-datasets/\", \"../../Monolinguals\", \"../../Cross-lingula\", \"../../Bilexicons_Folder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37b403-bf55-461a-805c-59f6d72c7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_d, target_d, src_b, trg_b = data_wrapper.load_source_target(\"af\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6187127-8ddb-483d-8cf4-1ffc409c7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_prep, t_prep = data_wrapper.train_test_split(80, 80, source_d, target_d, src_b, trg_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd647d-7e26-417b-89d4-5ee53b6538b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"The size of source train {len(s_prep[0])}, dev {len(s_prep[1])}, and test {len(s_prep[2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4b2f931-fc45-4d7b-bd39-8cbace529ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab_train = 13367\n",
    "vocab_label = 11417\n",
    "embedding_dim = 200\n",
    "units = 32\n",
    "n_sample = 10\n",
    "seq_length = 20\n",
    "X_enc = np.random.randint(0,vocab_train, (n_sample,20))\n",
    "X_dec = np.random.randint(0,vocab_label, (n_sample,seq_length))\n",
    "y = np.random.randint(0, 2, (n_sample,seq_length,vocab_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "baafd5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2454,  922, 4843, 7103, 1914, 8181, 1738, 9627, 3445, 7502, 9823,\n",
       "        937, 2052, 3801, 5009, 5072, 2419, 9986, 2625, 3463])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34082eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "src_embedding_sequences = Embedding(13367, 200, embeddings_initializer=\"uniform\")\n",
    "trg_embedding_sequences = Embedding(11417, 200, embeddings_initializer=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6c28226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method CustomAttention.call of <__main__.CustomAttention object at 0x7f0f04399cf8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method CustomAttention.call of <__main__.CustomAttention object at 0x7f0f04399cf8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# self.trg_seq_len \n",
    "src_inputs = Input(shape = (20,)) # [None, 20]\n",
    "src_embedding = src_embedding_sequences(src_inputs)\n",
    "# src_attention_layer = Attention(use_scale=False)([src_embedding, src_embedding])\n",
    "src_attention_layer, attn_weights = CustomAttention()(src_embedding)\n",
    "src_gru = GRU(5, return_sequences=True, return_state=True)\n",
    "src_out, src_state = src_gru(src_attention_layer)\n",
    "\n",
    "# Decoder\n",
    "# self.trg_seq_len - 1\n",
    "trg_inputs = Input(shape=(20,))\n",
    "trg_embedding = trg_embedding_sequences(trg_inputs)\n",
    "trg_attention_layer = Attention(use_scale=False)([trg_embedding, trg_embedding])\n",
    "trg_gru = GRU(5, return_sequences=True, return_state=True)\n",
    "trg_out, hidden_d = trg_gru(trg_attention_layer, initial_state = src_state)\n",
    "trg_dense = TimeDistributed(Dense(11417, activation='softmax'))    \n",
    "trg_pred = trg_dense(trg_out)\n",
    "\n",
    "# model compiling\n",
    "nmt = Model(inputs=[src_inputs, trg_inputs], outputs = trg_pred) \n",
    "nmt.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a46e5ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           multiple             2673400     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 200)      2283400     input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "custom_attention_6 (CustomAtten ((None, 20, 200), (N 120000      embedding[14][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 20, 200)      0           embedding_1[2][0]                \n",
      "                                                                 embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_11 (GRU)                    [(None, 20, 5), (Non 3090        custom_attention_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "gru_12 (GRU)                    [(None, 20, 5), (Non 3090        attention_8[0][0]                \n",
      "                                                                 gru_11[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 20, 11417)    68502       gru_12[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,151,482\n",
      "Trainable params: 5,151,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nmt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2672f0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 10 samples\n",
      "Epoch 1/3\n",
      "10/10 [==============================] - 1s 52ms/sample - loss: 53362.7188 - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 0s 5ms/sample - loss: 53362.3789 - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 0s 5ms/sample - loss: 53362.0195 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0eb46bf0b8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt.fit([X_enc, X_dec], y, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "66c45ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "we = nmt.layers[4].get_weights()\n",
    "nmt.layers[4].set_weights(we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024eb744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9549351d",
   "metadata": {},
   "source": [
    "# Get layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c240ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.wrappers.TimeDistributed at 0x7f28143c0eb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt.layers[4] # attention 1\n",
    "nmt.layers[6] # gru 1\n",
    "nmt.layers[5] # attention 2\n",
    "nmt.layers[7] # gru 2\n",
    "nmt.layers[8] # distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc03aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_en_wei = nmt.layers[4].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67866b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_en_wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8291a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_en_weights   = nmt.layers[6].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3104dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_en_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab4be4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           multiple             2673400     input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 1, 200)       0           embedding[6][0]                  \n",
      "                                                                 embedding[6][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_13 (GRU)                    [(None, 1, 5), (None 3090        attention_14[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,676,490\n",
      "Trainable params: 2,676,490\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get Model Weights encoder\n",
    "attention_en_wei = nmt.layers[4].get_weights()\n",
    "gru_en_weights   = nmt.layers[6].get_weights()\n",
    "\n",
    "\n",
    "# redefine models\n",
    "# The Encoder definition\n",
    "en_inputs = Input(shape= (1,))\n",
    "src_embeddings  =  src_embedding_sequences(en_inputs)\n",
    "# src_attention  = Attention(use_scale=False)\n",
    "src_attention  = Attention(use_scale=False)([src_embeddings, src_embeddings])\n",
    "src_gru = GRU(5, return_sequences=True, return_state=True)\n",
    "src_out, src_state = src_gru(src_attention)\n",
    "encoder = Model(inputs=en_inputs, outputs=src_state)\n",
    "src_gru.set_weights(gru_en_weights)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6dd0cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 11417)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             2283400     input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 11417, 200)   0           embedding_1[4][0]                \n",
      "                                                                 embedding_1[4][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_17 (GRU)                    [(None, 11417, 5), ( 3090        attention_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 11417, 11417) 68502       gru_17[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,354,992\n",
      "Trainable params: 2,354,992\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attention_de_wei = nmt.layers[5].get_weights()\n",
    "gru_de_weights   = nmt.layers[7].get_weights()\n",
    "dense_de_wei     = nmt.layers[8].get_weights()\n",
    "\n",
    "# The decoder definition\n",
    "de_inputs = Input(shape= (11417,))\n",
    "de_state_in = Input(shape= (5,))\n",
    "trg_embeddings  =  trg_embedding_sequences(de_inputs)\n",
    "# trg_attention  = Attention(use_scale=False)\n",
    "trg_attention  = Attention(use_scale=False)([trg_embeddings, trg_embeddings])\n",
    "trg_gru         =  GRU(5, return_sequences=True, return_state=True)\n",
    "trg_out, trg_state = trg_gru(trg_attention)\n",
    "de_dense = Dense(vocab_label , activation='softmax') \n",
    "de_pred = de_dense(trg_out)\n",
    "decoder = Model(inputs=[de_inputs, de_state_in], outputs=[de_pred, trg_state])\n",
    "trg_gru.set_weights(gru_de_weights)\n",
    "de_dense.set_weights(dense_de_wei)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5b6fb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f0e9c49bc50>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.layers[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d07ff0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_onehot(word, vocab_size):\n",
    "    \n",
    "    \"\"\" Converts a single word into onehot representation.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (tf.keras.preprocessing.text.Tokenizer): Tensorflow tokenizer object.\n",
    "        word (str): Word to be tokenized and onehot encoded.\n",
    "        vocab_size (int): Number of words in the whole vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        de_onhot (list): Onehot representation of given word.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    de_seq = np.array([1010])\n",
    "    de_onehot = to_categorical(de_seq, num_classes=vocab_size).reshape( 1, vocab_size)  \n",
    "    \n",
    "    return de_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3f4bd215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10860,  7946,  1590,  8705, 10317,  7425,  9808,  3605,  2060,\n",
       "        2173,  5110,  9221,   191, 11266,  6587, 10092,  9569,  7266,\n",
       "         912,  4628])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dec[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6dad458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = word_to_onehot(\"word\", 11417)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bdbeabf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fbb3a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 11417)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aa74d42f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-4982eb76359f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# en_st[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# de_seq = word_to_onehot(it_tok, \"sos\", it_vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mde_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# it_sent = \"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# # for i in range(it_seq_len):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "en_seq = X_enc[0]\n",
    "en_st = encoder.predict(np.array(en_seq))\n",
    "# en_st[0]\n",
    "# de_seq = word_to_onehot(it_tok, \"sos\", it_vocab_size)\n",
    "de_seq = w\n",
    "# it_sent = \"\"\n",
    "# # for i in range(it_seq_len):         \n",
    "de_prob, en_st = decoder.predict([de_seq, en_st])\n",
    "index = np.argmax(de_prob[0, :], axis=-1)\n",
    "#     de_w = it_tok.index_word[index]\n",
    "#     de_seq = word_to_onehot(it_tok, de_w, it_vocab_size) \n",
    "#     if de_w == 'eos': break\n",
    "#     it_sent += de_w + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5bbe31cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11112"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0e95b9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11112"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[11112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e925c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model Weights encoder\n",
    "attention_en_wei = nmt.layers[4].get_weights()\n",
    "gru_en_weights   = nmt.layers[6].get_weights()\n",
    "\n",
    "\n",
    "# redefine models\n",
    "# The Encoder definition\n",
    "en_inputs = Input(shape= (seq_length,))\n",
    "src_embeddings  =  src_emb_layer(en_inputs)\n",
    "# src_attention  = Attention(use_scale=False)\n",
    "src_attention  = Attention(use_scale=False)([src_embeddings, src_embeddings])\n",
    "src_gru = GRU(gru_units, return_sequences=True, return_state=True)\n",
    "src_out, src_state = src_gru(src_attention)\n",
    "encoder = Model(inputs=en_inputs, outputs=src_state)\n",
    "\n",
    "# set weights\n",
    "# src_attention.set_weights(attention_en_wei)\n",
    "# src_attention_output = src_attention([src_embeddings, src_embeddings])\n",
    "# src_gru = GRU(gru_units, return_sequences=True, return_state=True)\n",
    "# src_out, src_state = src_gru(src_attention_output)\n",
    "# encoder = Model(inputs=en_inputs, outputs=src_state)\n",
    "src_gru.set_weights(gru_en_weights)\n",
    "print(encoder.summary())\n",
    "# get decoder weights\n",
    "attention_de_wei = nmt.layers[5].get_weights()\n",
    "gru_de_weights   = nmt.layers[7].get_weights()\n",
    "dense_de_wei     = nmt.layers[8].get_weights()\n",
    "\n",
    "# The decoder definition\n",
    "de_inputs = Input(shape= (decoder_seq_len,))\n",
    "de_state_in = Input(shape= (gru_units,))\n",
    "trg_embeddings  =  trg_emb_layer(de_inputs)\n",
    "# trg_attention  = Attention(use_scale=False)\n",
    "trg_attention  = Attention(use_scale=False)([trg_embeddings, trg_embeddings])\n",
    "trg_gru         =  GRU(gru_units, return_sequences=True, return_state=True)\n",
    "trg_out, trg_state = trg_gru(trg_attention)\n",
    "de_dense = Dense(len(trg_data_tokenizer.word_index) + 1, activation='softmax') \n",
    "de_pred = de_dense(trg_out)\n",
    "decoder = Model(inputs=[de_inputs, de_state_in], outputs=[de_pred, trg_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec549f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Wrapper:\n",
    "    def Encoder_Decoder(self, src_emb_layer, trg_emb_layer):\n",
    "        # Encoder\n",
    "        # self.trg_seq_len \n",
    "        self.src_inputs = Input(shape = (20,)) # [None, 20]\n",
    "        self.src_embedding_sequences = src_emb_layer(self.src_inputs) # [None, 20, 200]\n",
    "        self.src_attention_layer = Attention(use_scale=False)([self.src_embedding_sequences, self.src_embedding_sequences])\n",
    "        self.src_gru = GRU(5, return_sequences=True, return_state=True)\n",
    "        self.src_out, self.src_state = self.src_gru(self.src_attention_layer)\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        # self.trg_seq_len - 1\n",
    "        self.trg_inputs = Input(shape=(20,))\n",
    "        self.trg_embedding_sequences = trg_emb_layer(self.trg_inputs)\n",
    "        self.trg_attention_layer = Attention(use_scale=False)([self.trg_embedding_sequences, self.trg_embedding_sequences])\n",
    "        self.trg_gru = GRU(5, return_sequences=True, return_state=True)\n",
    "        self.trg_out, hidden_d = self.trg_gru(self.trg_attention_layer, initial_state = self.src_state)\n",
    "        self.trg_dense = TimeDistributed(Dense(11418, activation='softmax'))    \n",
    "        self.trg_pred = self.trg_dense(self.trg_out)\n",
    "        \n",
    "        # model compiling\n",
    "        nmt = Model(inputs=[self.src_inputs, self.trg_inputs], outputs = self.trg_pred) \n",
    "        nmt.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "        \n",
    "        print(nmt.summary())\n",
    "        return nmt\n",
    "\n",
    "    # return attention src\n",
    "    def get_Attention_1(self,):\n",
    "            return self.src_attention_layer\n",
    "    \n",
    "    def get_Attention_2(self,):\n",
    "            return self.trg_attention_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70189b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Model_Wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "781d7d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 200)      2673400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 200)      2283400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 20, 200)      0           embedding[0][0]                  \n",
      "                                                                 embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 20, 200)      0           embedding_1[0][0]                \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, 20, 5), (Non 3090        attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, 20, 5), (Non 3090        attention_1[0][0]                \n",
      "                                                                 gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 20, 11418)    68508       gru_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,031,488\n",
      "Trainable params: 5,031,488\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nmt = obj.Encoder_Decoder(src_embedding_sequences, trg_embedding_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fb0a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = obj.get_Attention_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "101ee409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'attention/MatMul_1:0' shape=(?, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f11faad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method CustomAttention.call of <__main__.CustomAttention object at 0x7f0e9c549b38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method CustomAttention.call of <__main__.CustomAttention object at 0x7f0e9c549b38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        multiple                  2673400   \n",
      "_________________________________________________________________\n",
      "custom_attention_9 (CustomAt ((None, 1, 200), (None, 1 120000    \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 [(None, 1, 5), (None, 5)] 3090      \n",
      "=================================================================\n",
      "Total params: 2,796,490\n",
      "Trainable params: 2,796,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_en_weights   = nmt.layers[6].get_weights()\n",
    "\n",
    "\n",
    "# redefine models\n",
    "# The Encoder definition\n",
    "en_inputs = Input(shape= (1,))\n",
    "src_embeddings  =  src_embedding_sequences(en_inputs)\n",
    "# src_attention  = Attention(use_scale=False)\n",
    "# src_attention = Attention(use_scale=False, return_attention_scores=True)([src_embeddings, src_embeddings])\n",
    "src_attention, attn_weights = CustomAttention()(src_embeddings)\n",
    "# src_attention = att\n",
    "src_gru = GRU(5, return_sequences=True, return_state=True)\n",
    "src_out, src_state = src_gru(src_attention)\n",
    "encoder = Model(inputs=en_inputs, outputs=src_state)\n",
    "src_gru.set_weights(gru_en_weights)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "919478cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.02739114,  0.05869712,  0.04125505, ..., -0.0414524 ,\n",
       "         -0.01321885, -0.00778681],\n",
       "        [ 0.005569  , -0.01787361, -0.00258336, ...,  0.11083088,\n",
       "         -0.0481852 ,  0.06598014],\n",
       "        [-0.05304746,  0.0226344 ,  0.01155334, ..., -0.03073661,\n",
       "         -0.06675497, -0.01322996],\n",
       "        ...,\n",
       "        [-0.05502572,  0.03625637, -0.00631793, ..., -0.005086  ,\n",
       "          0.05157558,  0.03848467],\n",
       "        [-0.06956178, -0.07117637,  0.08264324, ..., -0.02385855,\n",
       "         -0.09164532,  0.04490397],\n",
       "        [-0.01559316, -0.04234954,  0.0796225 , ...,  0.10675714,\n",
       "         -0.00382941,  0.03039161]], dtype=float32),\n",
       " array([[-0.04815827,  0.04404342,  0.04731196, ..., -0.02803381,\n",
       "          0.04054996,  0.0157356 ],\n",
       "        [ 0.01667677, -0.1002574 , -0.02866348, ..., -0.00522761,\n",
       "         -0.03821593, -0.05233166],\n",
       "        [ 0.00280667, -0.06686115, -0.013961  , ...,  0.009312  ,\n",
       "         -0.0434284 ,  0.0596703 ],\n",
       "        ...,\n",
       "        [ 0.07349294,  0.03683276,  0.02412139, ..., -0.06202654,\n",
       "         -0.04561849,  0.03981544],\n",
       "        [-0.02037137, -0.05670867, -0.01154517, ..., -0.02433327,\n",
       "          0.02094992,  0.04288715],\n",
       "        [-0.05061288,  0.03366988,  0.03678271, ...,  0.02570257,\n",
       "         -0.08418073,  0.02968366]], dtype=float32),\n",
       " array([[ 0.02479656,  0.01257279, -0.06974106, ...,  0.04577565,\n",
       "          0.02532384, -0.02416739],\n",
       "        [ 0.0297157 ,  0.0063568 , -0.09150248, ..., -0.05064204,\n",
       "          0.00017032,  0.05408928],\n",
       "        [-0.05013368, -0.05993733, -0.00981633, ..., -0.02592435,\n",
       "          0.08167868,  0.00569205],\n",
       "        ...,\n",
       "        [-0.08588624, -0.06935599, -0.01546757, ...,  0.06183706,\n",
       "         -0.00268863, -0.02807659],\n",
       "        [-0.04193179, -0.00120117,  0.01855143, ..., -0.02963839,\n",
       "         -0.03639115, -0.02082047],\n",
       "        [-0.01362146, -0.02876258,  0.02444368, ..., -0.0662444 ,\n",
       "          0.02929578, -0.01598649]], dtype=float32)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d16d85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.recurrent.GRU at 0x7f0ecc5b07b8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.layers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0586c953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94fc41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = obj.get_Attention_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fefb596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'attention_9/MatMul_1:0' shape=(?, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb3df9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'attention_10/MatMul_1:0' shape=(?, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_attention_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c45304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W_q = self.add_weight(\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='query_weights'\n",
    "        )\n",
    "        self.W_k = self.add_weight(\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='key_weights'\n",
    "        )\n",
    "        self.W_v = self.add_weight(\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='value_weights'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = tf.matmul(inputs, self.W_q)\n",
    "        k = tf.matmul(inputs, self.W_k)\n",
    "        v = tf.matmul(inputs, self.W_v)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_scores = tf.nn.softmax(attn_scores, axis=-1)\n",
    "\n",
    "        # Apply attention scores to values\n",
    "        output = tf.matmul(attn_scores, v)\n",
    "        return output, attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d6a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
