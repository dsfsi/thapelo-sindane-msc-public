{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This code demonstrates how you can use monolingual and cross-lingual embeddings to train machine learning models for downstream tasks (Part of Speech Tagging or Named Entity Recognition). In this code you will observe the following:\n",
        "* How to visualize monolingual and cross-lingual embeddings\n",
        "* How to train a sequance to sequance model with cross-lingual and monolingual embeddings. In this example code we used Bidirectional Long Short Term Memory (BiLSTM), however you can do the same with RNN, LSTM, GRU, and BiGRU (as commented in the code)\n",
        "* We start with Part of Speech Tagging."
      ],
      "metadata": {
        "id": "EZz-egnKs7rq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL3za5z9rgLx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from gensim.models import KeyedVectors\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, ReduceLROnPlateau\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import sys\n",
        "import nltk\n",
        "import re\n",
        "# Download the punkt tokenizer if not already downloaded\n",
        "#nltk.download('punkt')\n",
        "import random\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "#from tf.keras.callbacks import LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n",
        "    '''\n",
        "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
        "    '''\n",
        "    def schedule(epoch):\n",
        "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
        "\n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self,X_train, Y_train, X_val, Y_val):\n",
        "        self.X_train = X_train\n",
        "        self.Y_train = Y_train\n",
        "        self.X_val = X_val\n",
        "        self.Y_val = Y_val\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Predict on training data\n",
        "        y_train_pred = self.model.predict(self.X_train)\n",
        "        y_train_pred_flat = y_train_pred.argmax(axis=-1).flatten()\n",
        "        y_train_true_flat = self.Y_train.argmax(axis=-1).flatten()\n",
        "\n",
        "        # Predict on validation data\n",
        "        y_val_pred = self.model.predict(self.X_val)\n",
        "        y_val_pred_flat = y_val_pred.argmax(axis=-1).flatten()\n",
        "        y_val_true_flat = self.Y_val.argmax(axis=-1).flatten()\n",
        "\n",
        "        # Calculate precision, recall, and F1 score for training data\n",
        "        train_precision = precision_score(y_train_true_flat, y_train_pred_flat, average='weighted')\n",
        "        train_recall = recall_score(y_train_true_flat, y_train_pred_flat, average='weighted')\n",
        "        train_f1 = f1_score(y_train_true_flat, y_train_pred_flat, average='weighted')\n",
        "\n",
        "        # Calculate precision, recall, and F1 score for validation data\n",
        "        val_precision = precision_score(y_val_true_flat, y_val_pred_flat, average='weighted')\n",
        "        val_recall = recall_score(y_val_true_flat, y_val_pred_flat, average='weighted')\n",
        "        val_f1 = f1_score(y_val_true_flat, y_val_pred_flat, average='weighted')\n",
        "\n",
        "        # Print metrics\n",
        "        print(f'\\nEpoch {epoch + 1} Metrics:')\n",
        "        print(f'Training Precision: {train_precision:.4f}, Training Recall: {train_recall:.4f}, Training F1 Score: {train_f1:.4f}')\n",
        "        print(f'Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, Validation F1 Score: {val_f1:.4f}\\n')\n",
        "\n",
        "        # Add metrics to logs for TensorBoard or other monitoring tools\n",
        "        logs['train_precision'] = train_precision\n",
        "        logs['train_recall'] = train_recall\n",
        "        logs['train_f1'] = train_f1\n",
        "        logs['val_precision'] = val_precision\n",
        "        logs['val_recall'] = val_recall\n",
        "        logs['val_f1'] = val_f1\n",
        "\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "def f1_rec_prec(passed_model, X_test , Y_test):\n",
        "        Y_pred = passed_model.predict(X_test)\n",
        "        # Convert predictions and ground truth to 1D arrays\n",
        "        Y_pred_flat = Y_pred.argmax(axis=-1).flatten()\n",
        "        Y_true_flat = Y_test.argmax(axis=-1).flatten()\n",
        "\n",
        "        # Calculate precision, recall, and F1 score\n",
        "        precision = precision_score(Y_true_flat, Y_pred_flat, average='weighted')\n",
        "        recall = recall_score(Y_true_flat, Y_pred_flat, average='weighted')\n",
        "        f1 = f1_score(Y_true_flat, Y_pred_flat, average='weighted')\n",
        "        return f1, recall, precision\n",
        "\n",
        "# inference\n",
        "def sequence_to_tag(list_of_indices, reverse_word_map_param):\n",
        "    # Looking up words in dictionary\n",
        "    print(len(list_of_indices[0]))\n",
        "    sentence_labels = [np.argmax(token_probs) for token_probs in list_of_indices]\n",
        "    #pred_sequence_no_padding = [token for token in list_of_indices.argmax(axis=-1) if token != 0]\n",
        "    words = [reverse_word_map_param[letter] for letter in sentence_labels if letter != 0]\n",
        "    return(words)\n",
        "\n",
        "def sequence_to_text(list_of_indices, reverse_word_map_param):\n",
        "    # Looking up words in dictionary\n",
        "    words = [reverse_word_map_param[letter] for letter in list(list_of_indices) if letter != 0]\n",
        "\n",
        "    return(words)\n",
        "\n",
        "def inference(passed_model,  X_test,  tokenizer_tag, tokenizer_words, filename ):\n",
        "              Y_pred = passed_model.predict(X_test)\n",
        "              # loop through sentences\n",
        "              reverse_word_map = dict(map(reversed, tokenizer_tag.word_index.items()))\n",
        "              print(len(reverse_word_map))\n",
        "              rev_text_word_map = dict(map(reversed, tokenizer_words.word_index.items()))\n",
        "\n",
        "              # write to file\n",
        "              with open(filename, \"w\") as f_name:\n",
        "                        for i, sent in enumerate(X_test):\n",
        "                                      #s_split = sent.split()\n",
        "                                      s_split  = sequence_to_text( sent, rev_text_word_map)\n",
        "                                      print(s_split)\n",
        "                                      predic_text = sequence_to_tag(Y_pred[i], reverse_word_map)\n",
        "                                      print(predic_text)\n",
        "                                      for p, pred in enumerate(predic_text):\n",
        "                                                    try:\n",
        "                                                            f_name.write(s_split[p] + \" \" + pred + \"\\n\")\n",
        "                                                    except:\n",
        "                                                            pass\n",
        "                                      f_name.write(\"\\n\")\n",
        "\n",
        "# In[5]:\n",
        "def confusion_m_plot(passed_model, X_test , Y_test, lang, label_tokenizer, m_name):\n",
        "            Y_pred = passed_model.predict(X_test)\n",
        "            # Convert predictions and ground truth to 1D arrays\n",
        "            Y_pred_flat = Y_pred.argmax(axis=-1).flatten()\n",
        "            Y_true_flat = Y_test.argmax(axis=-1).flatten()\n",
        "\n",
        "            index_to_label = {index: label for label, index in label_tokenizer.word_index.items()}\n",
        "            #decoded_label_Pred = [index for index in Y_pred_flat.argmax()]\n",
        "            #print(decoded_label_Pred)\n",
        "            #decoded_label_Y = [index for index in Y_true_flat.argmax()]\n",
        "\n",
        "\n",
        "            label_orig = list(index_to_label.values())\n",
        "            # Generate the confusion matrix\n",
        "            conf_matrix = confusion_matrix(Y_true_flat, Y_pred_flat)\n",
        "            conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "            # Plot the confusion matrix using seaborn\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(conf_matrix, annot=True, fmt='.2g', cmap='Blues', xticklabels=label_orig, yticklabels=label_orig)\n",
        "            plt.title('Confusion Matrix')\n",
        "            plt.xlabel('Predicted Labels')\n",
        "            plt.ylabel('True Labels')\n",
        "            plt.savefig(confus_path + m_name + lang + \".jpeg\", format='jpeg', dpi=300)\n",
        "#             plt.show()\n",
        "\n",
        "\n",
        "\n",
        "class Data_Model_Wrapper_Code_Switch:\n",
        "        def __init__(self,root_data, root_mono, root_cross, path_to_destination_eda):\n",
        "                    # initialize paths\n",
        "                    self.data_path     = root_data\n",
        "                    self.path_to_mono_emb = root_mono + \"/\"\n",
        "                    self.path_to_cross_emb = root_cross + \"/\"\n",
        "                    self.path_to_eda_output = path_to_destination_eda + \"/\"\n",
        "\n",
        "        # load embeddings\n",
        "        def get_emb_mat(self, src_language, trg_language):\n",
        "                    # loading cross_lingual embeddings\n",
        "                    s_t_folder = src_language + \"_\" + trg_language\n",
        "                    s_t_file_src = src_language + \"_\" + trg_language + \"-src-vcmp.txt\"\n",
        "                    s_t_file_trg = src_language + \"_\" + trg_language + \"-trg-vcmp.txt\"\n",
        "\n",
        "                    src_x_emb_file_p = self.path_to_cross_emb + s_t_folder + \"/\" + s_t_file_src\n",
        "                    trg_x_emb_file_p = self.path_to_cross_emb + s_t_folder + \"/\" + s_t_file_trg\n",
        "\n",
        "                    # load src cross emb\n",
        "                    # src_embeddings_index = {}\n",
        "                    # with open(src_x_emb_file_p) as f:\n",
        "                    #                 for  line in f:\n",
        "                    #                         word, coefs = line.split(maxsplit=1)\n",
        "                    #                         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "                    #                         src_embeddings_index[word] = coefs\n",
        "\n",
        "                    # load target cross embeddings\n",
        "                    trg_embeddings_index = {}\n",
        "                    with open(trg_x_emb_file_p) as f:\n",
        "                                    for  line in f:\n",
        "                                            word, coefs = line.split(maxsplit=1)\n",
        "                                            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "                                            trg_embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "                    src_vocab = self.word_tokenizer.word_index\n",
        "                    self.src_num_tokens = len(self.word_tokenizer.word_index) + 1\n",
        "                    #print(src_num_tokens)\n",
        "                    embedding_dim =  200\n",
        "                    hits_src = 0\n",
        "                    misses_src = 0\n",
        "                    hits_trg = 0\n",
        "                    misses_trg = 0\n",
        "                    input_size = 200\n",
        "                    # self.code_switch_embedding_matrix = np.zeros((self.src_num_tokens, embedding_dim))\n",
        "                    # for word, i in word2idx_all.items():\n",
        "                    self.trg_embedding_matrix = np.zeros((self.src_num_tokens, embedding_dim))\n",
        "                    # embeddings_index    =  self.code_switch_emb(src_embeddings_index, trg_embeddings_index)\n",
        "                    for word in list(src_vocab.keys()):\n",
        "                                    # trg emb\n",
        "                                    embedding_vector = trg_embeddings_index.get(word)\n",
        "                                    if embedding_vector is not None:\n",
        "                                        self.trg_embedding_matrix[src_vocab[word]] = embedding_vector\n",
        "                                        # self.code_switch_embedding_matrix[src_vocab[word]] = embedding_vector\n",
        "                                        hits_trg +=1\n",
        "                                    else:\n",
        "\n",
        "                                        misses_trg += 1\n",
        "\n",
        "    #                   return code_switch_layer\n",
        "\n",
        "        # load data\n",
        "        def load_train_dev_test(self):\n",
        "                            # load train csv and devide data into train, dev and test.\n",
        "                            try:\n",
        "                                train = open(os.path.join(self.data_path ,'train.txt'),'r').readlines()\n",
        "                            except FileNotFoundError:\n",
        "                                train = ''\n",
        "                            try:\n",
        "                                dev = open(os.path.join(self.data_path ,'dev.txt'),'r').readlines()\n",
        "                            except FileNotFoundError:\n",
        "                                dev = ''\n",
        "\n",
        "                            try:\n",
        "                                test = open(os.path.join(self.data_path ,'test.txt'),'r').readlines()\n",
        "                            except FileNotFoundError:\n",
        "                                test = ''\n",
        "\n",
        "                            new = []\n",
        "                            sent = []\n",
        "                            tag = []\n",
        "                            X = []\n",
        "                            Y = []\n",
        "                            for t in train:\n",
        "                                if t.strip() != '':\n",
        "                                    split = t.strip().split(' ')\n",
        "                                    if len(split) > 1:\n",
        "                                        #print(split)\n",
        "                                        sent.append(split[0])\n",
        "                                        tag.append(split[1])\n",
        "\n",
        "                                else:\n",
        "                                    new_s = ' '.join(sent)\n",
        "                                    new_t = ' '.join(tag)\n",
        "                                    X.append(sent)\n",
        "                                    Y.append(tag)\n",
        "                                    new.append([new_s,new_t])\n",
        "                                    sent = []\n",
        "                                    tag = []\n",
        "\n",
        "                            sent = []\n",
        "                            tag = []\n",
        "                            x_l = len(X)\n",
        "\n",
        "                            if dev != '':\n",
        "                                for d in dev:\n",
        "                                        if d.strip() != '':\n",
        "                                            split = d.strip().split(' ')\n",
        "                                            sent.append(split[0])\n",
        "                                            tag.append(split[1])\n",
        "                                        else:\n",
        "                                            new_s = ' '.join(sent)\n",
        "                                            new_t = ' '.join(tag)\n",
        "                                            X.append(sent)\n",
        "                                            Y.append(tag)\n",
        "                                            new.append([new_s,new_t])\n",
        "                                            sent = []\n",
        "                                            tag = []\n",
        "\n",
        "                            d_l = len(X) - x_l\n",
        "\n",
        "                            sent = []\n",
        "                            tag = []\n",
        "\n",
        "                            if test != '':\n",
        "                                for tt in test:\n",
        "                                    if tt.strip() != '':\n",
        "                                        split = tt.strip().split(' ')\n",
        "                                        sent.append(split[0])\n",
        "                                        tag.append(split[1])\n",
        "                                    else:\n",
        "                                        new_s = ' '.join(sent)\n",
        "                                        new_t = ' '.join(tag)\n",
        "                                        X.append(sent)\n",
        "                                        Y.append(tag)\n",
        "                                        new.append([new_s,new_t])\n",
        "                                        sent = []\n",
        "                                        tag = []\n",
        "                            self.X_text        =  [\" \".join(x) for x in X[x_l + d_l:]]\n",
        "\n",
        "\n",
        "                            num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
        "                            num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
        "                            print(\"Total number of tagged sentences: {}\".format(len(X)))\n",
        "                            print(\"Vocabulary size: {}\".format(num_words))\n",
        "                            print(\"Total number of tags: {}\".format(num_tags))\n",
        "\n",
        "\n",
        "\n",
        "                            self.word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
        "                            self.word_tokenizer.fit_on_texts(X)                    # fit tokeniser on data\n",
        "                            X_encoded = self.word_tokenizer.texts_to_sequences(X)  # use the tokeniser to encode input sequence\n",
        "\n",
        "                            # encode Y\n",
        "                            self.tag_tokenizer = Tokenizer()\n",
        "                            self.tag_tokenizer.fit_on_texts(Y)\n",
        "                            Y_encoded = self.tag_tokenizer.texts_to_sequences(Y)\n",
        "\n",
        "                            # make sure that each sequence of input and output is same length\n",
        "                            different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]\n",
        "                            print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))\n",
        "\n",
        "                            lengths = [len(seq) for seq in X_encoded]\n",
        "                            self.MAX_SEQ_LENGTH = max(lengths)  # sequences greater than 100 in length will be truncated\n",
        "                            X_padded = pad_sequences(X_encoded, maxlen=self.MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "                            Y_padded = pad_sequences(Y_encoded, maxlen=self.MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "\n",
        "                            # assign padded sequences to X and Y\n",
        "                            X, Y = X_padded, Y_padded\n",
        "                            Y = to_categorical(Y)\n",
        "                            Y = [array[:, 1:] for array in Y]\n",
        "                            # Convert the result back to a list of NumPy arrays\n",
        "                            Y = np.array([np.array(arr) for arr in Y])\n",
        "\n",
        "\n",
        "                            self.X_train       =  X[:x_l]\n",
        "                            self.Y_train       =  Y[:x_l]\n",
        "                            self.X_test        =  X[x_l + d_l: ]\n",
        "                            self.Y_test        =  Y[x_l + d_l: ]\n",
        "\n",
        "                            self.X_validation  =   X[x_l: x_l + d_l]\n",
        "                            self.Y_validation  =   Y[x_l: x_l + d_l]\n",
        "\n",
        "\n",
        "                            # print number of samples in each set\n",
        "                            print(\"TRAINING DATA\")\n",
        "                            print('Shape of input sequences: {}'.format(self.X_train.shape))\n",
        "                            print('Shape of output sequences: {}'.format(self.Y_train.shape))\n",
        "                            print(\"-\"*50)\n",
        "                            print(\"VALIDATION DATA\")\n",
        "                            print('Shape of input sequences: {}'.format(self.X_validation.shape))\n",
        "                            print('Shape of output sequences: {}'.format(self.Y_validation.shape))\n",
        "                            print(\"-\"*50)\n",
        "                            print(\"TESTING DATA\")\n",
        "                            print('Shape of input sequences: {}'.format(self.X_test.shape))\n",
        "                            print('Shape of output sequences: {}'.format(self.Y_test.shape))\n",
        "                            self.NUM_CLASSES = Y.shape[2]\n",
        "                            #print(Y.shape)\n",
        "    #                       return X_train, Y_train, X_validation, Y_validation, X_test,Y_test\n",
        "\n",
        "\n",
        "        # training models\n",
        "        def seq_to_seq_models(self,epochs, batch_s, type, destination, embed_type, source, target_lang, EMBEDDING_SIZE):\n",
        "\n",
        "                            # Assuming you have self.X_validation and self.Y_validation for validation data\n",
        "                            f1_callback = F1ScoreCallback(self.X_train, self.Y_train, self.X_validation, self.Y_validation)\n",
        "                            '''\n",
        "                            lr_sched_rn = step_decay_schedule(initial_lr=1e-3, decay_factor=0.45, step_size=4)\n",
        "                            # Define a callback to modify the learning rate dynamically\n",
        "                            earl_rnn = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
        "\n",
        "                            # create RNN architecture\n",
        "                            opt_rnn = tf.keras.optimizers.Adam()\n",
        "                            self.rnn_model = Sequential()\n",
        "                            # create embedding layer - usually the first layer in text problems\n",
        "                            self.rnn_model.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            weights       = [self.code_switch_embedding_matrix],      # word embedding matrix\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            # add an RNN layer which contains 64 RNN cells\n",
        "                            self.rnn_model.add(SimpleRNN(64,\n",
        "                                        return_sequences=True  # True - return whole sequence; False - return single output of the end of the sequence\n",
        "                            ))\n",
        "\n",
        "                            # add time distributed (output at each sequence) layer\n",
        "                            self.rnn_model.add(Dense(32, activation='relu'))\n",
        "                            self.rnn_model.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.rnn_model.compile(loss      =  'categorical_crossentropy',\n",
        "                                            optimizer =  opt_rnn,\n",
        "                                            metrics   =  ['acc'])\n",
        "                            print('\\n','************* TRAINING RNN *************','\\n')\n",
        "                            rnn_training = self.rnn_model.fit(self.X_train, self.Y_train, batch_size=batch_s, epochs=epochs, validation_data=(self.X_validation, self.Y_validation),callbacks=[f1_callback, lr_sched_rn, earl_rnn], verbose = 2)\n",
        "                            img_rnn =  img_dir + \"X_rnn_acc_plot_\" +source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(rnn_training.history['acc'])\n",
        "                            plt.plot(rnn_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            #plt.legend(['rnn_train', 'rnn_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_rnn, format='eps')\n",
        "                            plt.show()\n",
        "                            '''\n",
        "                            '''\n",
        "                            lr_sched_lstm = step_decay_schedule(initial_lr=1e-3, decay_factor=0.6, step_size=8)\n",
        "                            earl_lstm = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
        "                            # create LSTM architecture10\n",
        "                            #opt_lstm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "                            opt_lstm = tf.keras.optimizers.Adam()\n",
        "                            self.lstm_model = Sequential()\n",
        "                            self.lstm_model.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            weights       = [self.code_switch_embedding_matrix],      # word embedding matrix\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            self.lstm_model.add(LSTM(64, return_sequences=True))\n",
        "                            self.lstm_model.add(Dense(32, activation='relu'))\n",
        "                            self.lstm_model.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.lstm_model.compile(loss      =  'categorical_crossentropy',\n",
        "                                            optimizer =  opt_lstm,\n",
        "                                            metrics   =  ['acc'])\n",
        "                            print('\\n','************* TRAINING LSTM *************','\\n')\n",
        "                            lstm_training = self.lstm_model.fit(self.X_train, self.Y_train, batch_size=batch_s, epochs=epochs, validation_data=(self.X_validation, self.Y_validation),callbacks=[f1_callback,lr_sched_lstm, earl_lstm], verbose = 2)\n",
        "                            img_lstm = img_dir + \"X_lstm_acc_plot_\" +source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(lstm_training.history['acc'])\n",
        "                            plt.plot(lstm_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            #plt.legend(['lstm_train', 'lstm_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_lstm, format='eps')\n",
        "                            plt.show()\n",
        "\n",
        "\n",
        "                            # create GRU architecture\n",
        "                            lr_sched_gru = step_decay_schedule(initial_lr=1e-2, decay_factor=0.5, step_size=4)\n",
        "                            earl_gru = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
        "                            #opt_gru = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "                            opt_gru = tf.keras.optimizers.Adam()\n",
        "                            self.gru_model = Sequential()\n",
        "                            self.gru_model.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            weights       = [self.code_switch_embedding_matrix],      # word embedding matrix\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            self.gru_model.add(GRU(64, return_sequences=True))\n",
        "                            self.gru_model.add(Dense(32, activation='relu'))\n",
        "                            self.gru_model.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.gru_model.compile(loss='categorical_crossentropy',\n",
        "                                        optimizer= opt_gru,\n",
        "                                        metrics=['acc'])\n",
        "                            print('\\n','************* TRAINING GRU *************','\\n')\n",
        "                            gru_training = self.gru_model.fit(self.X_train, self.Y_train, batch_size=batch_s, epochs=epochs, validation_data=(self.X_validation, self.Y_validation), callbacks=[f1_callback, lr_sched_gru, earl_gru], verbose = 2 )\n",
        "                            img_gru = img_dir + \"X_gru_acc_plot_\" +source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(gru_training.history['acc'])\n",
        "                            plt.plot(gru_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            #plt.legend(['gru_train', 'gru_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_gru, format='eps')\n",
        "                            plt.show()\n",
        "\n",
        "\n",
        "                            # create BiGRU\n",
        "                            lr_sched_bigru = step_decay_schedule(initial_lr=1e-3, decay_factor=0.65, step_size=5)\n",
        "                            red_lr  = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=3, min_lr=0.0001)\n",
        "                            earl_bigru = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "                            #opt_bigru = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "                            opt_bigru = tf.keras.optimizers.Adam()\n",
        "                            self.bidirect_gru = Sequential()\n",
        "                            self.bidirect_gru.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            weights       = [self.code_switch_embedding_matrix],      # word embedding matrix\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            self.bidirect_gru.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "                            self.bidirect_gru.add(Dense(32, activation='relu'))\n",
        "                            self.bidirect_gru.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.bidirect_gru.compile(loss='categorical_crossentropy',\n",
        "                                        optimizer=opt_bigru,\n",
        "                                        metrics=['acc'])\n",
        "                            print('\\n','************* TRAINING BIDIRECTIONAL GRU *************','\\n')\n",
        "                            bidigru_training = self.bidirect_gru.fit(self.X_train, self.Y_train, batch_size=batch_s, epochs=epochs, validation_data=(self.X_validation, self.Y_validation),callbacks=[f1_callback, lr_sched_bigru,red_lr], verbose = 2)\n",
        "                            img_bigru = img_dir + \"X_bigru_acc_plot_\" +source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(bidigru_training.history['acc'])\n",
        "                            plt.plot(bidigru_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            #plt.legend(['gru_train', 'gru_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_bigru, format='eps')\n",
        "                            plt.show()\n",
        "                            '''\n",
        "\n",
        "                            # create BiLSTM architecture\n",
        "                            lr_sched = step_decay_schedule(initial_lr=1e-5, decay_factor=0.3, step_size=9)\n",
        "                            earl_bilstm = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "                            opt_bilstm = tf.keras.optimizers.Adam()\n",
        "                            self.bidirect_model = Sequential()\n",
        "                            self.bidirect_model.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            # weights       = [self.code_switch_embedding_matrix],      # word embedding matrix\n",
        "                            weights       = [self.trg_embedding_matrix],\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            self.bidirect_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "                            self.bidirect_model.add(Dense(32, activation='relu'))\n",
        "                            self.bidirect_model.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.bidirect_model.compile(loss='categorical_crossentropy',\n",
        "                                        optimizer=opt_bilstm,\n",
        "                                        metrics=['acc'])\n",
        "                            print('\\n','************* TRAINING BIDIRECTIONAL LSTM *************','\\n')\n",
        "                            bidirect_training = self.bidirect_model.fit(self.X_train, self.Y_train, batch_size=batch_s, epochs=epochs, validation_data=(self.X_validation, self.Y_validation),callbacks=[f1_callback, lr_sched,earl_bilstm], verbose = 2)\n",
        "\n",
        "                            img_bilstm = img_dir + \"X_bilstm_acc_plot_\" + source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(bidirect_training.history['acc'])\n",
        "                            plt.plot(bidirect_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            plt.legend(['rnn_train', 'rnn_test', 'lstm_train', 'lstm_test','gru_train', 'gru_test', 'bigru_train', 'bigru_test','bils_train', 'bils_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_bilstm, format='eps')\n",
        "                            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "                            with open(os.path.join(destination, embed_type + '_' + type +'_results_f1'+ source+ \"-\" + target_lang + 'run1' + '.txt'),'w') as writer:\n",
        "                                            '''\n",
        "                                            loss, accuracy = self.rnn_model.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            f, r, p = f1_rec_prec(self.rnn_model, self.X_test, self.Y_test)\n",
        "                                            writer.write('RNN')\n",
        "                                            writer.write('\\n')\n",
        "                                            writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            writer.write('\\n\\n')\n",
        "\n",
        "\n",
        "                                            loss, accuracy = self.lstm_model.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            f, r, p = f1_rec_prec(self.lstm_model, self.X_test, self.Y_test)\n",
        "                                            writer.write('LSTM')\n",
        "                                            writer.write('\\n')\n",
        "                                            writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            writer.write('\\n\\n')\n",
        "\n",
        "                                            loss, accuracy = self.gru_model.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            f, r, p = f1_rec_prec(self.gru_model, self.X_test, self.Y_test)\n",
        "                                            writer.write('GRU')\n",
        "                                            writer.write('\\n')\n",
        "                                            writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            writer.write('\\n\\n')\n",
        "\n",
        "                                            loss, accuracy = self.bidirect_gru.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            f, r, p = f1_rec_prec(self.bidirect_gru, self.X_test, self.Y_test)\n",
        "                                            writer.write('Bidirectional GRU')\n",
        "                                            writer.write('\\n')\n",
        "                                            writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            writer.write('\\n\\n')\n",
        "                                            '''\n",
        "\n",
        "\n",
        "                                            loss, accuracy = self.bidirect_model.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            f, r, p = f1_rec_prec(self.bidirect_model, self.X_test, self.Y_test)\n",
        "                                            writer.write('Bidirectional LSTM')\n",
        "                                            writer.write('\\n')\n",
        "                                            writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            writer.write('\\n\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "oUgCzeVqr1uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dG4tF7u_2Xlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Cross-lingual Embeddings\n",
        "* This takes a bit of time to align the embeddings"
      ],
      "metadata": {
        "id": "3dNw0Uwi2R3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import time\n",
        "modules_path = os.path.join('../src')\n",
        "sys.path.append(modules_path)\n",
        "from crosslingual import align_lex, plot_bilex, pca_bilex, tsne_plot\n",
        "import pickle\n",
        "# plot_bilex, pca_bilex"
      ],
      "metadata": {
        "id": "6uYfpL-i2RUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = ['en'] #here you would list all 11 languages\n",
        "target_lang = ['xho'] #here you would list all 11 languages\n",
        "path_in = '../ext_data/M2M_MT_train_Embeddings/en_xho'\n",
        "path_lex = './bilexicons/en_xho'\n",
        "path_out = './processed/VecMap' #where aligned lexicon data will be stored and read from again"
      ],
      "metadata": {
        "id": "Np3AZAcOXc2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# align vectors\n",
        "for o in source_lang:\n",
        "    for p in target_lang:\n",
        "        if p != o:\n",
        "            file_source = o+'_'+p+'-src-vcmp.txt'\n",
        "            file_target = o+'_'+p+'-trg-vcmp.txt'\n",
        "            file_source = os.path.join(path_in,file_source)\n",
        "            file_target = os.path.join(path_in,file_target)\n",
        "            bilex = o+'-'+p+'-bilex.txt'\n",
        "            bilex = os.path.join(path_lex,bilex)\n",
        "\n",
        "            align_lex(file_source, file_target, bilex, path_out, o, p, 'vecmap') # specificy if it is cca or vecmap, since cca has no header in txt file whereas vecmap does\n",
        "\n"
      ],
      "metadata": {
        "id": "VFXPevbeXc-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = ['en'] #here you would list all 11 languages\n",
        "target_lang = ['xho'] #here you would list all 11 languages\n",
        "path_in = './processed/VecMap'\n",
        "path_out = './figures'   #where aligned lexicon data will be stored and read from again"
      ],
      "metadata": {
        "id": "MryJVp0q20pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for o in source_lang:\n",
        "    for p in target_lang:\n",
        "        if p != o:\n",
        "            print('Processing ********     ',o,' - to - ',p)\n",
        "            l1 = o\n",
        "            l2 = p\n",
        "            name1=l1+'2'+l2+'_'+l1+'vec.txt'\n",
        "            name2=l1+'2'+l2+'_'+l2+'vec.txt'\n",
        "\n",
        "            name1 = os.path.join(path_in,name1)\n",
        "            name2 = os.path.join(path_in,name2)\n",
        "\n",
        "            with open(name1, 'rb') as fp:\n",
        "                vecs=pickle.load(fp)\n",
        "            with open(name2, 'rb') as fp:\n",
        "                vectar=pickle.load(fp)\n",
        "\n",
        "            name1=l1+'2'+l2+'_'+l1+'.txt'\n",
        "            name2=l1+'2'+l2+'_'+l2+'.txt'\n",
        "\n",
        "            name1 = os.path.join(path_in,name1)\n",
        "            name2 = os.path.join(path_in,name2)\n",
        "\n",
        "            with open(name1, 'rb') as fp:\n",
        "                source1=pickle.load(fp)\n",
        "            with open(name2, 'rb') as fp:\n",
        "                target1=pickle.load(fp)\n",
        "\n",
        "            fig_name = l1+'2'+l2+' ('+str(start)+'-'+str(end)+').png'\n",
        "            fig_name = os.path.join(path_out,fig_name)\n",
        "\n",
        "            a1,b1,c1,d1 = plot_bilex(source1, target1, vecs, vectar, fig_name, start=start,end=end)\n",
        "\n",
        "            #while True:\n",
        "            #    try:\n",
        "            #        a1,b1,c1,d1 = plot_bilex(source1, target1, vecs, vectar, fig_name, start=start,end=end)\n",
        "            #    except ValueError:\n",
        "            #        print('Redoing ******** ',o,' - to - ',p)\n",
        "                    #a1,b1,c1,d1 = plot_bilex(source1, target1, vecs, vectar, fig_name, start=start,end=end)\n",
        "                #else:\n",
        "                #    break\n",
        "\n",
        "            name1=l1+'2'+l2+'_cos.txt'\n",
        "            name1 = os.path.join(path_in,name1)\n",
        "\n",
        "            with open(name1, 'wb') as fp:\n",
        "                source1=pickle.dump(d1,fp)"
      ],
      "metadata": {
        "id": "c6QFXSs23FBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-lingual Training"
      ],
      "metadata": {
        "id": "Szg45MH9s1Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Languages = [\"nso\", \"tsn\", \"xho\", \"zul\", \"sot\"]"
      ],
      "metadata": {
        "id": "2Z9f3Z4K1CWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose languages\n",
        "param1 = \"en\"\n",
        "param2 = \"xho\"\n",
        "\n",
        "data_path = \"./data/xho\"\n",
        "cross_emb_path = \"../ext_data/M2M_MT_train_Embeddings\"                       # Path to embeddings\n",
        "mono_path      = \"../../../../\"\n",
        "model_destination_path = \"/../../../../ext_data/thapelo/\"                                         # path to output directory\n",
        "img_dir = \"../ext_data/\"                               # path to plots\n",
        "output_text = \"./evaluation_crosslingual.txt\"\n",
        "\n",
        "# Train params\n",
        "epochs = 100\n",
        "batch_s = 418\n",
        "emb_size = 200\n",
        "type = 'vecmap'\n",
        "embed_type = 'crosslingual'\n",
        "destination_reports = \"../ext_data/\"\n",
        "confus_path = \"../ext_data/\""
      ],
      "metadata": {
        "id": "eak6a-Tq08Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training.\n",
        "if __name__=='__main__':\n",
        "            class_obj = Data_Model_Wrapper_Code_Switch(data_path, mono_path, cross_emb_path, model_destination_path)\n",
        "            class_obj.load_train_dev_test()\n",
        "            class_obj.get_emb_mat(param1, param2)\n",
        "            class_obj.seq_to_seq_models(epochs, batch_s, type, destination_reports, embed_type, param1, param2, emb_size)\n",
        "\n",
        "            inference(class_obj.bidirect_model, class_obj.X_test,  class_obj.tag_tokenizer,class_obj.word_tokenizer,output_text )\n",
        "            #confusion_m_plot(class_obj.bidirect_model, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'bi_lstm')\n",
        "            #confusion_m_plot(class_obj.lstm_model, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'lstm')\n",
        "            #confusion_m_plot(class_obj.gru_model, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'gru')\n",
        "            #confusion_m_plot(class_obj.bidirect_gru, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'bi_gru')\n",
        "            #confusion_m_plot(class_obj.rnn_model, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'rnn')\n",
        "\"\"\"# Monolingual embeddings Training\"\"\""
      ],
      "metadata": {
        "id": "durkwH7asz2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "4AskPp7XTxL2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PUXcUyUqTwQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oHKwM6BZ5I-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monolingual embeddings Training"
      ],
      "metadata": {
        "id": "K9Cc8tvI5MED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_Model_Wrapper_Monolingual:\n",
        "        def __init__(self,root_data, root_mono, root_cross, path_to_destination_eda):\n",
        "                    # initialize paths\n",
        "#                     self.source_data_p = root_data + \"/\"\n",
        "#                     self.target_data_p = root_data + \"/\"\n",
        "                    self.data_path     = root_data\n",
        "                    self.path_to_mono_emb = root_mono + \"/\"\n",
        "                    self.path_to_cross_emb = root_cross + \"/\"\n",
        "                    self.path_to_eda_output = path_to_destination_eda + \"/\"\n",
        "\n",
        "        # def merge the two emb\n",
        "        def code_switch_emb(self, dict1, dict2):\n",
        "                        combined_dict = dict1.copy()\n",
        "                        for key, value in dict2.items():\n",
        "                            if key in combined_dict:\n",
        "                                # Calculate the average of lists if the key is present in both dictionaries\n",
        "                                combined_dict[key] = [(a + b) / 2 for a, b in zip(combined_dict[key], value)]\n",
        "                            else:\n",
        "                                # Add the key-value pair if it's not present in the first dictionary\n",
        "                                combined_dict[key] = value\n",
        "\n",
        "                        return combined_dict\n",
        "\n",
        "        # load embeddings\n",
        "        def get_emb_mat(self, src_language, trg_language):\n",
        "                    # loading cross_lingual embeddings\n",
        "\n",
        "                    directory_trg = self.path_to_mono_emb + \t trg_language + \"/\"\n",
        "                    file_names_trg = [f for f in os.listdir(directory_trg) if os.path.isfile(os.path.join(directory_trg, f))]\n",
        "                    directory_src = self.path_to_mono_emb +  src_language + \"/\"\n",
        "                    file_names_src = [f for f in os.listdir(directory_src) if os.path.isfile(os.path.join(directory_src, f))]\n",
        "\n",
        "                    src_x_emb_file_p = directory_src + file_names_src[0]\n",
        "                    trg_x_emb_file_p =  directory_trg   + file_names_trg[0]\n",
        "\n",
        "\n",
        "                    # load src cross emb\n",
        "                    # src_embeddings_index = {}\n",
        "                    # with open(src_x_emb_file_p) as f:\n",
        "                    #                 for  line in f:\n",
        "                    #                         word, coefs = line.split(maxsplit=1)\n",
        "                    #                         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "                    #                         src_embeddings_index[word] = coefs\n",
        "\n",
        "                    # load target cross embeddings\n",
        "                    trg_embeddings_index = {}\n",
        "                    with open(trg_x_emb_file_p) as f:\n",
        "                                    for  line in f:\n",
        "                                            word, coefs = line.split(maxsplit=1)\n",
        "                                            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "                                            trg_embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "                    src_vocab = self.word_tokenizer.word_index\n",
        "                    self.src_num_tokens = len(self.word_tokenizer.word_index) + 1\n",
        "                    #print(src_num_tokens)\n",
        "                    embedding_dim =  200\n",
        "                    hits_src = 0\n",
        "                    misses_src = 0\n",
        "                    hits_trg = 0\n",
        "                    misses_trg = 0\n",
        "                    input_size = 200\n",
        "                    self.code_switch_embedding_matrix = np.zeros((self.src_num_tokens, embedding_dim))\n",
        "                    # for word, i in word2idx_all.items():\n",
        "                    self.trg_embedding_matrix = np.zeros((self.src_num_tokens, embedding_dim))\n",
        "                    # embeddings_index    =  self.code_switch_emb(src_embeddings_index, trg_embeddings_index)\n",
        "                    for word in list(src_vocab.keys()):\n",
        "                                    # trg emb\n",
        "                                    embedding_vector = trg_embeddings_index.get(word)\n",
        "                                    if embedding_vector is not None:\n",
        "                                        self.trg_embedding_matrix[src_vocab[word]] = embedding_vector\n",
        "                                        # self.code_switch_embedding_matrix[src_vocab[word]] = embedding_vector\n",
        "                                        hits_trg +=1\n",
        "                                    else:\n",
        "\n",
        "                                        misses_trg += 1\n",
        "#                     self.code_switch_layer = Embedding(self.src_num_tokens, embedding_dim, input_length=20, embeddings_initializer = keras.initializers.Constant(self.code_switch_embedding_matrix))\n",
        "                    #print(self.code_switch_layer)\n",
        "\n",
        "\n",
        "    #                   return code_switch_layer\n",
        "\n",
        "        # load data\n",
        "        def load_train_dev_test(self):\n",
        "                            # load datasets\n",
        "                            try:\n",
        "                                train = open(os.path.join(self.data_path ,'train.txt'),'r').readlines()\n",
        "                            except FileNotFoundError:\n",
        "                                train = ''\n",
        "                            try:\n",
        "                                dev = open(os.path.join(self.data_path ,'dev.txt'),'r').readlines()\n",
        "                            except FileNotFoundError:\n",
        "                                dev = ''\n",
        "\n",
        "                            try:\n",
        "                                test = open(os.path.join(self.data_path ,'test.txt'),'r').readlines()\n",
        "                            except FileNotFoundError:\n",
        "                                test = ''\n",
        "\n",
        "\n",
        "\n",
        "                            new = []\n",
        "                            sent = []\n",
        "                            tag = []\n",
        "                            X = []\n",
        "                            Y = []\n",
        "                            for t in train:\n",
        "                                if t.strip() != '':\n",
        "                                    split = t.strip().split(' ')\n",
        "                                    if len(split) > 1:\n",
        "                                        #print(split)\n",
        "                                        sent.append(split[0])\n",
        "                                        tag.append(split[1])\n",
        "\n",
        "                                else:\n",
        "                                    new_s = ' '.join(sent)\n",
        "                                    new_t = ' '.join(tag)\n",
        "                                    X.append(sent)\n",
        "                                    Y.append(tag)\n",
        "                                    new.append([new_s,new_t])\n",
        "                                    sent = []\n",
        "                                    tag = []\n",
        "\n",
        "                            sent = []\n",
        "                            tag = []\n",
        "\n",
        "                            if dev != '':\n",
        "                                for d in dev:\n",
        "                                        if d.strip() != '':\n",
        "                                            split = d.strip().split(' ')\n",
        "                                            sent.append(split[0])\n",
        "                                            tag.append(split[1])\n",
        "                                        else:\n",
        "                                            new_s = ' '.join(sent)\n",
        "                                            new_t = ' '.join(tag)\n",
        "                                            X.append(sent)\n",
        "                                            Y.append(tag)\n",
        "                                            new.append([new_s,new_t])\n",
        "                                            sent = []\n",
        "                                            tag = []\n",
        "\n",
        "                            sent = []\n",
        "                            tag = []\n",
        "\n",
        "                            if test != '':\n",
        "                                for tt in test:\n",
        "                                    if tt.strip() != '':\n",
        "                                        split = tt.strip().split(' ')\n",
        "                                        sent.append(split[0])\n",
        "                                        tag.append(split[1])\n",
        "                                    else:\n",
        "                                        new_s = ' '.join(sent)\n",
        "                                        new_t = ' '.join(tag)\n",
        "                                        X.append(sent)\n",
        "                                        Y.append(tag)\n",
        "                                        new.append([new_s,new_t])\n",
        "                                        sent = []\n",
        "                                        tag = []\n",
        "\n",
        "\n",
        "                            num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
        "                            num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
        "                            print(\"Total number of tagged sentences: {}\".format(len(X)))\n",
        "                            print(\"Vocabulary size: {}\".format(num_words))\n",
        "                            print(\"Total number of tags: {}\".format(num_tags))\n",
        "\n",
        "\n",
        "\n",
        "                            self.word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
        "                            self.word_tokenizer.fit_on_texts(X)                    # fit tokeniser on data\n",
        "                            X_encoded = self.word_tokenizer.texts_to_sequences(X)  # use the tokeniser to encode input sequence\n",
        "\n",
        "                            # encode Y\n",
        "                            self.tag_tokenizer = Tokenizer()\n",
        "                            self.tag_tokenizer.fit_on_texts(Y)\n",
        "                            Y_encoded = self.tag_tokenizer.texts_to_sequences(Y)\n",
        "\n",
        "                            # make sure that each sequence of input and output is same length\n",
        "                            different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]\n",
        "                            print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))\n",
        "\n",
        "                            lengths = [len(seq) for seq in X_encoded]\n",
        "                            self.MAX_SEQ_LENGTH = max(lengths)  # sequences greater than 100 in length will be truncated\n",
        "                            X_padded = pad_sequences(X_encoded, maxlen=self.MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "                            Y_padded = pad_sequences(Y_encoded, maxlen=self.MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "\n",
        "                            # assign padded sequences to X and Y\n",
        "                            X, Y = X_padded, Y_padded\n",
        "                            Y = to_categorical(Y)\n",
        "                            Y = [array[:, 1:] for array in Y]\n",
        "                            # Convert the result back to a list of NumPy arrays\n",
        "                            Y = np.array([np.array(arr) for arr in Y])\n",
        "\n",
        "                            x_l, d_l = len(train), len(dev)\n",
        "\n",
        "                            self.X_train       =  X[:x_l]\n",
        "                            self.Y_train       =  Y[:x_l]\n",
        "                            self.X_test        =  X[x_l + d_l: ]\n",
        "                            self.Y_test        =  Y[x_l + d_l: ]\n",
        "\n",
        "                            self.X_validation  =   X[x_l: x_l + d_l]\n",
        "                            self.Y_validation  =   Y[x_l: x_l + d_l]\n",
        "\n",
        "\n",
        "                            # print number of samples in each set\n",
        "                            print(\"TRAINING DATA\")\n",
        "                            print('Shape of input sequences: {}'.format(self.X_train.shape))\n",
        "                            print('Shape of output sequences: {}'.format(self.Y_train.shape))\n",
        "                            print(\"-\"*50)\n",
        "                            print(\"VALIDATION DATA\")\n",
        "                            print('Shape of input sequences: {}'.format(self.X_validation.shape))\n",
        "                            print('Shape of output sequences: {}'.format(self.Y_validation.shape))\n",
        "                            print(\"-\"*50)\n",
        "                            print(\"TESTING DATA\")\n",
        "                            print('Shape of input sequences: {}'.format(self.X_test.shape))\n",
        "                            print('Shape of output sequences: {}'.format(self.Y_test.shape))\n",
        "                            self.NUM_CLASSES = Y.shape[2]\n",
        "                            #print(Y.shape)\n",
        "    #                       return X_train, Y_train, X_validation, Y_validation, X_test,Y_test\n",
        "\n",
        "\n",
        "        # training models\n",
        "        def seq_to_seq_models(self,epochs, batch_s, type, destination, embed_type, source, target_lang, EMBEDDING_SIZE):\n",
        "\n",
        "                            # Assuming you have self.X_validation and self.Y_validation for validation data\n",
        "                            f1_callback = F1ScoreCallback(self.X_train, self.Y_train, self.X_validation, self.Y_validation)\n",
        "                            '''\n",
        "                            #lr_sched_rn = step_decay_schedule(initial_lr=1e-5, decay_factor=0.38, step_size=2)\n",
        "                            # Define a callback to modify the learning rate dynamically\n",
        "                            earl_rnn = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "                            reduce_lr_rnn = ReduceLROnPlateau(monitor='val_loss', factor=0.14, patience=3, min_lr=0.000001)\n",
        "                            # create RNN architecture\n",
        "                            opt_rnn = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "                            self.rnn_model = Sequential()\n",
        "                            # create embedding layer - usually the first layer in text problems\n",
        "                            self.rnn_model.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            weights       = [self.code_switch_embedding_matrix],      # word embedding matrix\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            # add an RNN layer which contains 64 RNN cells\n",
        "                            self.rnn_model.add(SimpleRNN(64,\n",
        "                                        return_sequences=True  # True - return whole sequence; False - return single output of the end of the sequence\n",
        "                            ))\n",
        "\n",
        "                            # add time distributed (output at each sequence) layer\n",
        "                            self.rnn_model.add(Dense(32, activation='relu'))\n",
        "                            self.rnn_model.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.rnn_model.compile(loss      =  'categorical_crossentropy',\n",
        "                                            optimizer =  opt_rnn,\n",
        "                                            metrics   =  ['acc'])\n",
        "                            print('\\n','************* TRAINING RNN *************','\\n')\n",
        "                            rnn_training = self.rnn_model.fit(self.X_train, self.Y_train, batch_size=282, epochs=epochs, validation_data=(self.X_validation, self.Y_validation), callbacks=[f1_callback, earl_rnn, reduce_lr_rnn], verbose = 2)\n",
        "                            img_rnn =  img_dir + \"X_rnn_acc_plot_\" +source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(rnn_training.history['acc'])\n",
        "                            plt.plot(rnn_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            #plt.legend(['rnn_train', 'rnn_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_rnn, format='eps')\n",
        "                            plt.show()\n",
        "\n",
        "\n",
        "                            #lr_sched_lstm = step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=6)\n",
        "                            earl_lstm = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "                            # create LSTM architecture10\n",
        "                            opt_lstm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "                            reduce_lr_lstm = ReduceLROnPlateau(monitor='val_loss', factor=0.68, patience=6, min_lr=0.000001)\n",
        "                            #opt_lstm = tf.keras.optimizers.Adam()\n",
        "                            self.lstm_model = Sequential()\n",
        "                            self.lstm_model.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            weights       = [self.code_switch_embedding_matrix],      # word embedding matrix\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            self.lstm_model.add(LSTM(64, return_sequences=True))\n",
        "                            self.lstm_model.add(Dense(32, activation='relu'))\n",
        "                            self.lstm_model.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.lstm_model.compile(loss      =  'categorical_crossentropy',\n",
        "                                            optimizer =  opt_lstm,\n",
        "                                            metrics   =  ['acc'])\n",
        "                            print('\\n','************* TRAINING LSTM *************','\\n')\n",
        "                            lstm_training = self.lstm_model.fit(self.X_train, self.Y_train, batch_size=batch_s, epochs=epochs, validation_data=(self.X_validation, self.Y_validation), callbacks=[f1_callback,  reduce_lr_lstm, earl_lstm], verbose = 2)\n",
        "                            img_lstm = img_dir + \"X_lstm_acc_plot_\" +source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(lstm_training.history['acc'])\n",
        "                            plt.plot(lstm_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            #plt.legend(['lstm_train', 'lstm_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_lstm, format='eps')\n",
        "                            plt.show()\n",
        "\n",
        "\n",
        "                            # create GRU architecture\n",
        "                            lr_sched_gru = step_decay_schedule(initial_lr=1e-5, decay_factor=0.1, step_size=8)\n",
        "                            earl_gru = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "                            #opt_gru = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "                            opt_gru = tf.keras.optimizers.Adam()\n",
        "                            self.gru_model = Sequential()\n",
        "                            self.gru_model.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            weights       = [self.code_switch_embedding_matrix],      # word embedding matrix\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            self.gru_model.add(GRU(64, return_sequences=True))\n",
        "                            self.gru_model.add(Dense(32, activation='relu'))\n",
        "                            self.gru_model.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.gru_model.compile(loss='categorical_crossentropy',\n",
        "                                        optimizer= opt_gru,\n",
        "                                        metrics=['acc'])\n",
        "                            print('\\n','************* TRAINING GRU *************','\\n')\n",
        "                            gru_training = self.gru_model.fit(self.X_train, self.Y_train, batch_size=batch_s, epochs=epochs, validation_data=(self.X_validation, self.Y_validation), callbacks=[f1_callback, earl_gru, lr_sched_gru], verbose = 2 )\n",
        "                            img_gru = img_dir + \"X_gru_acc_plot_\" +source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(gru_training.history['acc'])\n",
        "                            plt.plot(gru_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            #plt.legend(['gru_train', 'gru_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_gru, format='eps')\n",
        "                            plt.show()\n",
        "\n",
        "                            # create BiGRU\n",
        "                            lr_sched_bigru = step_decay_schedule(initial_lr=1e-5, decay_factor=0.2, step_size=10)\n",
        "                            earl_bigru = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "                            #opt_bigru = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "                            opt_bigru = tf.keras.optimizers.Adam()\n",
        "                            self.bidirect_gru = Sequential()\n",
        "                            self.bidirect_gru.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            weights       = [self.code_switch_embedding_matrix],      # word embedding matrix\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            self.bidirect_gru.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "                            self.bidirect_gru.add(Dense(32, activation='relu'))\n",
        "                            self.bidirect_gru.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.bidirect_gru.compile(loss='categorical_crossentropy',\n",
        "                                        optimizer=opt_bigru,\n",
        "                                        metrics=['acc'])\n",
        "                            print('\\n','************* TRAINING BIDIRECTIONAL GRU *************','\\n')\n",
        "                            bidigru_training = self.bidirect_gru.fit(self.X_train, self.Y_train, batch_size=512, epochs=epochs, validation_data=(self.X_validation, self.Y_validation),callbacks=[f1_callback, earl_bigru,lr_sched_bigru], verbose = 2)\n",
        "                            img_bigru = img_dir + \"X_bigru_acc_plot_\" +source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(bidigru_training.history['acc'])\n",
        "                            plt.plot(bidigru_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            #plt.legend(['gru_train', 'gru_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_bigru, format='eps')\n",
        "                            plt.show()\n",
        "                            '''\n",
        "\n",
        "                            # create BiLSTM architecture\n",
        "                            lr_sched = step_decay_schedule(initial_lr=1e-5, decay_factor=0.3, step_size=10)\n",
        "                            earl_bilstm = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "                            opt_bilstm = tf.keras.optimizers.Adam()\n",
        "                            self.bidirect_model = Sequential()\n",
        "                            self.bidirect_model.add(Embedding(input_dim   =  self.src_num_tokens,         # vocabulary size - number of unique words in data\n",
        "                            output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                            input_length  =  self.MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                            weights       = [self.trg_embedding_matrix],      # word embedding matrix\n",
        "                            trainable     =  True                     # True - update the embeddings while training\n",
        "                            ))\n",
        "                            self.bidirect_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "                            self.bidirect_model.add(Dense(32, activation='relu'))\n",
        "                            self.bidirect_model.add(TimeDistributed(Dense(self.NUM_CLASSES, activation='softmax')))\n",
        "                            self.bidirect_model.compile(loss='categorical_crossentropy',\n",
        "                                        optimizer=opt_bilstm,\n",
        "                                        metrics=['acc'])\n",
        "                            print('\\n','************* TRAINING BIDIRECTIONAL LSTM *************','\\n')\n",
        "                            bidirect_training = self.bidirect_model.fit(self.X_train, self.Y_train, batch_size=batch_s, epochs=epochs, validation_data=(self.X_validation, self.Y_validation),callbacks=[f1_callback, lr_sched,earl_bilstm], verbose = 2)\n",
        "\n",
        "                            img_bilstm = img_dir + \"X_bilstm_acc_plot_\" + source + '-' + target_lang + \".eps\"\n",
        "                            # visualise training history\n",
        "                            plt.plot(bidirect_training.history['acc'])\n",
        "                            plt.plot(bidirect_training.history['val_acc'])\n",
        "                            plt.title('model accuracy')\n",
        "                            plt.ylabel('accuracy')\n",
        "                            plt.xlabel('epoch')\n",
        "                            plt.legend(['rnn_train', 'rnn_test', 'lstm_train', 'lstm_test','gru_train', 'gru_test', 'bigru_train', 'bigru_test','bils_train', 'bils_test'], loc=\"lower right\")\n",
        "                            plt.savefig(img_bilstm, format='eps')\n",
        "                            plt.show()\n",
        "\n",
        "                            with open(os.path.join(destination, embed_type + '_' + type +'_results_f1'+ source+ \"-\" + target_lang + 'run1' + '.txt'),'w') as writer:\n",
        "                                            # loss, accuracy = self.rnn_model.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            # f, r, p = f1_rec_prec(self.rnn_model, self.X_test, self.Y_test)\n",
        "                                            # writer.write('RNN')\n",
        "                                            # writer.write('\\n')\n",
        "                                            # writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            # writer.write('\\n\\n')\n",
        "\n",
        "                                            # loss, accuracy = self.lstm_model.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            # f, r, p = f1_rec_prec(self.lstm_model, self.X_test, self.Y_test)\n",
        "                                            # writer.write('LSTM')\n",
        "                                            # writer.write('\\n')\n",
        "                                            # writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            # writer.write('\\n\\n')\n",
        "\n",
        "                                            # loss, accuracy = self.gru_model.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            # f, r, p = f1_rec_prec(self.gru_model, self.X_test, self.Y_test)\n",
        "                                            # writer.write('GRU')\n",
        "                                            # writer.write('\\n')\n",
        "                                            # writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            # writer.write('\\n\\n')\n",
        "\n",
        "                                            # loss, accuracy = self.bidirect_gru.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            # f, r, p = f1_rec_prec(self.bidirect_gru, self.X_test, self.Y_test)\n",
        "                                            # writer.write('Bidirectional GRU')\n",
        "                                            # writer.write('\\n')\n",
        "                                            # writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            # writer.write('\\n\\n')\n",
        "\n",
        "                                            loss, accuracy = self.bidirect_model.evaluate(self.X_test, self.Y_test, verbose = 1)\n",
        "                                            f, r, p = f1_rec_prec(self.bidirect_model, self.X_test, self.Y_test)\n",
        "                                            writer.write('Bidirectional LSTM')\n",
        "                                            writer.write('\\n')\n",
        "                                            writer.write(\"Loss: {0},\\nAccuracy: {1}, \\nF1 Score: {2},  \\nRecall: {3},  \\nPrecision: {4} \".format(loss, accuracy, f, r, p))\n",
        "                                            writer.write('\\n\\n')"
      ],
      "metadata": {
        "id": "LodYw6Gl5PZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Languages = [\"nso\", \"tsn\", \"xho\", \"zul\", \"sot\"]"
      ],
      "metadata": {
        "id": "E5sHUoY-5xsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose languages\n",
        "param1 = \"en\"\n",
        "param2 = \"xho\"\n",
        "\n",
        "data_path = \"./data/xho\"\n",
        "cross_emb_path = \"../ext_data/M2M_MT_train_Embeddings\"                       # Path to embeddings\n",
        "mono_path      = \"../ext_data/M2M_mono_Embeddings/m2m-mono-embeddings\"\n",
        "model_destination_path = \"/../../../../ext_data/thapelo/\"                                         # path to output directory\n",
        "img_dir = \"../ext_data/\"                               # path to plots\n",
        "# Train params\n",
        "epochs = 100\n",
        "batch_s = 418\n",
        "emb_size = 200\n",
        "type = 'vecmap'\n",
        "embed_type = 'crosslingual'\n",
        "destination_reports = \"../ext_data/\"\n",
        "confus_path = \"../ext_data/\"\n",
        "output_text = \"./evaluation_monolingual.txt\""
      ],
      "metadata": {
        "id": "Frb8WBx45x4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "if __name__=='__main__':\n",
        "            class_obj = Data_Model_Wrapper_Monolingual(data_path, mono_path, cross_emb_path, model_destination_path)\n",
        "            class_obj.load_train_dev_test()\n",
        "            class_obj.get_emb_mat(param1, param2)\n",
        "            class_obj.seq_to_seq_models(epochs, batch_s, type, destination_reports, embed_type, param1, param2, emb_size)\n",
        "\n",
        "            confusion_m_plot(class_obj.bidirect_model, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'bi_lstm')\n",
        "            inference(class_obj.bidirect_model, class_obj.X_test, class_obj.tag_tokenizer,class_obj.word_tokenizer, output_text )\n",
        "\n",
        "            # confusion_m_plot(class_obj.lstm_model, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'lstm')\n",
        "            # confusion_m_plot(class_obj.gru_model, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'gru')\n",
        "            # confusion_m_plot(class_obj.bidirect_gru, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'bi_gru')\n",
        "            # confusion_m_plot(class_obj.rnn_model, class_obj.X_test, class_obj.Y_test, param1 + '-' + param2, class_obj.tag_tokenizer, 'rnn')"
      ],
      "metadata": {
        "id": "75GkmppX5ZqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LPaXSaz5JE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monolinual Plots"
      ],
      "metadata": {
        "id": "ozrU2H2zDfhW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZbh9MI8DbNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}